

@article{hooker2019benchmark,
  title={A benchmark for interpretability methods in deep neural networks},
  author={Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yin2022u,
  title={U-Net-Based medical image segmentation},
  author={Yin, Xiao-Xia and Sun, Le and Fu, Yuhan and Lu, Ruiliang and Zhang, Yanchun},
  journal={Journal of Healthcare Engineering},
  volume={2022},
  year={2022},
  publisher={Hindawi}
}

@article{sarker2021deep,
  title={Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions},
  author={Sarker, Iqbal H},
  journal={SN Computer Science},
  volume={2},
  number={6},
  pages={420},
  year={2021},
  publisher={Springer}
}


@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of big data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={SpringerOpen}
}

@article{samek2021explaining,
  title={Explaining deep neural networks and beyond: A review of methods and applications},
  author={Samek, Wojciech and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Anders, Christopher J and M{\"u}ller, Klaus-Robert},
  journal={Proceedings of the IEEE},
  volume={109},
  number={3},
  pages={247--278},
  year={2021},
  publisher={IEEE}
}


@inproceedings{doumard2022comparative,
  title={A comparative study of additive local explanation methods based on feature influences},
  author={Doumard, Emmanuel and Aligon, Julien and Escriva, Elodie and Excoffier, Jean-Baptiste and Monsarrat, Paul and Soul{\'e}-Dupuy, Chantal},
  booktitle={24th International Workshop on Design, Optimization, Languages and Analytical Processing of Big Data ((DOLAP 2022)},
  volume={3130},
  number={paper 4},
  pages={31--40},
  year={2022},
  organization={CEUR-WS. org}
}


@ARTICLE{9320473,
  author={Jung, Yeon-Jee and Han, Seung-Ho and Choi, Ho-Jin},
  journal={IEEE Access}, 
  title={Explaining CNN and RNN Using Selective Layer-Wise Relevance Propagation}, 
  year={2021},
  volume={9},
  number={},
  pages={18670-18681},
  doi={10.1109/ACCESS.2021.3051171}}


@misc{stergiou2021minds,
      title={The Mind's Eye: Visualizing Class-Agnostic Features of CNNs}, 
      author={Alexandros Stergiou},
      year={2021},
      eprint={2101.12447},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{zheng2022shapcam,
      title={Shap-CAM: Visual Explanations for Convolutional Neural Networks based on Shapley Value}, 
      author={Quan Zheng and Ziwei Wang and Jie Zhou and Jiwen Lu},
      year={2022},
      eprint={2208.03608},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{IVANOVS2021228,
title = {Perturbation-based methods for explaining deep neural networks: A survey},
journal = {Pattern Recognition Letters},
volume = {150},
pages = {228-234},
year = {2021},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2021.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167865521002440},
author = {Maksims Ivanovs and Roberts Kadikis and Kaspars Ozols},
keywords = {Deep learning, Explainable artificial intelligence, Perturbation-based methods},
abstract = {Deep neural networks (DNNs) have achieved state-of-the-art results in a broad range of tasks, in particular the ones dealing with the perceptual data. However, full-scale application of DNNs in safety-critical areas is hindered by their black box-like nature, which makes their inner workings nontransparent. As a response to the black box problem, the field of explainable artificial intelligence (XAI) has recently emerged and is currently rapidly growing. The present survey is concerned with perturbation-based XAI methods, which allow to explore DNN models by perturbing their input and observing changes in the output. We present an overview of the most recent research focusing on the differences and similarities in the applications of perturbation-based methods to different data types, from extensively studied perturbations of images to the just emerging research on perturbations of video, natural language, software code, and reinforcement learning entities.}
}

@article{brocki2022evaluation,
  title={Evaluation of interpretability methods and perturbation artifacts in deep neural networks},
  author={Brocki, Lennart and Chung, Neo Christopher},
  journal={arXiv preprint arXiv:2203.02928},
  year={2022}
}




@article{chlap2021review,
  title={A review of medical image data augmentation techniques for deep learning applications},
  author={Chlap, Phillip and Min, Hang and Vandenberg, Nym and Dowling, Jason and Holloway, Lois and Haworth, Annette},
  journal={Journal of Medical Imaging and Radiation Oncology},
  volume={65},
  number={5},
  pages={545--563},
  year={2021},
  publisher={Wiley Online Library}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@article{santos2022avoiding,
  title={Avoiding overfitting: A survey on regularization methods for convolutional neural networks},
  author={Santos, Claudio Filipi Gon{\c{c}}alves Dos and Papa, Jo{\~a}o Paulo},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--25},
  year={2022},
  publisher={ACM New York, NY}
}

@misc{balestriero2022effects,
      title={The Effects of Regularization and Data Augmentation are Class Dependent}, 
      author={Randall Balestriero and Leon Bottou and Yann LeCun},
      year={2022},
      eprint={2204.03632},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2020selectscale,
      title={SelectScale: Mining More Patterns from Images via Selective and Soft Dropout}, 
      author={Zhengsu Chen and Jianwei Niu and Xuefeng Liu and Shaojie Tang},
      year={2020},
      eprint={2012.15766},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{anas2017clinical,
  title={Clinical target-volume delineation in prostate brachytherapy using residual neural networks},
  author={Anas, Emran Mohammad Abu and Nouranian, Saman and Mahdavi, S Sara and Spadinger, Ingrid and Morris, William J and Salcudean, Septimu E and Mousavi, Parvin and Abolmaesumi, Purang},
  booktitle={Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20},
  pages={365--373},
  year={2017},
  organization={Springer}
}

@article{koszewicz2021use,
  title={The Use of Quantitative Sensation Testing to Identify the Physiological Differences Between the Median and Ulnar Nerves},
  author={Koszewicz, Magdalena and Szydlo, Mariusz and Gosk, Jerzy and Wieczorek, Malgorzata and Slotwinski, Krzysztof and Budrewicz, Slawomir},
  journal={Frontiers in Human Neuroscience},
  volume={15},
  pages={601322},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{Zhang2019ARO,
  title={A Review on Deep Learning in Medical Image Reconstruction},
  author={Haimiao Zhang and Bin Dong},
  journal={Journal of the Operations Research Society of China},
  year={2019},
  volume={8},
  pages={311-340}
}

@article{TSUNEKI2022312,
title = {Deep learning models in medical image analysis},
journal = {Journal of Oral Biosciences},
volume = {64},
number = {3},
pages = {312-320},
year = {2022},
issn = {1349-0079},
doi = {https://doi.org/10.1016/j.job.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1349007922000500},
author = {Masayuki Tsuneki},
keywords = {Medical image analysis, Computer-aided diagnosis, Computer vision, Deep learning, Artificial intelligence},
abstract = {Background
Deep learning is a state-of-the-art technology that has rapidly become the method of choice for medical image analysis. Its fast and robust object detection, segmentation, tracking, and classification of pathophysiological anatomical structures can support medical practitioners during routine clinical workflow. Thus, deep learning-based applications for diseases diagnosis will empower physicians and allow fast decision-making in clinical practice.
Highlight
Deep learning can be more robust with various features for differentiating classes, provided the training set is large and diverse for analysis. However, sufficient medical images for training sets are not always available from medical institutions, which is one of the major limitations of deep learning in medical image analysis. This review article presents some solutions for this issue and discusses efforts needed to develop robust deep learning-based computer-aided diagnosis applications for better clinical workflow in endoscopy, radiology, pathology, and dentistry.
Conclusion
The introduction of deep learning-based applications will enhance the traditional role of medical practitioners in ensuring accurate diagnoses and treatment in terms of precision, reproducibility, and scalability.}
}


@article{kim2020cycnn,
  title={Cycnn: A rotation invariant cnn using polar mapping and cylindrical convolution layers},
  author={Kim, Jinpyo and Jung, Wooekun and Kim, Hyungmo and Lee, Jaejin},
  journal={arXiv preprint arXiv:2007.10588},
  year={2020}
}



@article{lalonde2021capsules,
  title={Capsules for biomedical image segmentation},
  author={LaLonde, Rodney and Xu, Ziyue and Irmakci, Ismail and Jain, Sanjay and Bagci, Ulas},
  journal={Medical image analysis},
  volume={68},
  pages={101889},
  year={2021},
  publisher={Elsevier}
}


@article{PAN2022101767,
title = {Capsule network-based semantic segmentation model for thermal anomaly identification on building envelopes},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101767},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101767},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002257},
author = {Chenbin Pan and Jiyang Wang and Weiheng Chai and Burak Kakillioglu and Yasser {El Masri} and Eleanna Panagoulia and Norhan Bayomi and Kaiwen Chen and John E. Fernandez and Tarek Rakha and Senem Velipasalar},
keywords = {Building inspection, Thermal anomaly, Segmentation, Metric},
abstract = {Thermography technology is widely used to inspect thermal anomalies in building façade systems. Computer vision-based techniques provide opportunities to autonomously detect such heat anomalies to significantly improve the efficiency of decision-making for building envelope retrofitting and maintenance. In this work, we propose a novel Capsule Network-based deep learning model – CapsLab – that detects and identifies thermal anomalies by semantic segmentation. CapsLab is built based on our proposed prediction-tuning capsule (PT-Capsule) layer. Different from a traditional capsule layer, which consists of part-whole transformation and capsule-routing process, the proposed layer is composed of a prediction and tuning process, which helps decreasing the number of model parameters significantly. While the applicability of traditional Capsule Networks (CapsNets) has been limited to simpler tasks and smaller datasets due to their scalability issue, we can leverage the lightweight of the proposed PT-Capsule layer, and apply it to the semantic segmentation task. In this work, we also employ our previously presented performance metric, referred to as the Anomaly Identification Metric (AIM) (Kakillioglua et al. 2021), to evaluate the segmentation outputs. Traditional performance metrics do not accurately reflect the true performance of the segmentation models in thermal anomaly identification due to the high subjectivity in the annotation process and higher overlap ratio sensitivity of the standard metrics. AIM, on the other hand, is robust to these drawbacks. Experimental results show, both qualitatively and quantitatively, that our proposed segmentation method can effectively segment the thermal anomalies. Specifically, our model provides 9.38\% and 13.53\% improvements over the baseline model – DeepLabV3+ – based on traditional mIoU score and the AIM score, respectively, while requiring less model parameters and less computation at the same time. In addition, the scores that the AIM metric generates better align with the scores provided by building performance experts.}
}


@ARTICLE{9200334,
  author={Kang, Seokho},
  journal={IEEE Access}, 
  title={Rotation-Invariant Wafer Map Pattern Classification With Convolutional Neural Networks}, 
  year={2020},
  volume={8},
  number={},
  pages={170650-170658},
  doi={10.1109/ACCESS.2020.3024603}}


@article{hong2022multi,
  title={A Multi-Scale Convolutional Neural Network for Rotation-Invariant Recognition},
  author={Hong, Tzung-Pei and Hu, Ming-Jhe and Yin, Tang-Kai and Wang, Shyue-Liang},
  journal={Electronics},
  volume={11},
  number={4},
  pages={661},
  year={2022},
  publisher={MDPI}
}



@Article{tomography8060241,
AUTHOR = {Loizidou, Kosmia and Skouroumouni, Galateia and Nikolaou, Christos and Pitris, Costas},
TITLE = {A Review of Computer-Aided Breast Cancer Diagnosis Using Sequential Mammograms},
JOURNAL = {Tomography},
VOLUME = {8},
YEAR = {2022},
NUMBER = {6},
PAGES = {2874--2892},
URL = {https://www.mdpi.com/2379-139X/8/6/241},
PubMedID = {36548533},
ISSN = {2379-139X},
ABSTRACT = {Radiologists assess the results of mammography, the key screening tool for the detection of breast cancer, to determine the presence of malignancy. They, routinely, compare recent and prior mammographic views to identify changes between the screenings. In case a new lesion appears in a mammogram, or a region is changing rapidly, it is more likely to be suspicious, compared to a lesion that remains unchanged and it is usually benign. However, visual evaluation of mammograms is challenging even for expert radiologists. For this reason, various Computer-Aided Diagnosis (CAD) algorithms are being developed to assist in the diagnosis of abnormal breast findings using mammograms. Most of the current CAD systems do so using only the most recent mammogram. This paper provides a review of the development of methods to emulate the radiological approach and perform automatic segmentation and/or classification of breast abnormalities using sequential mammogram pairs. It begins with demonstrating the importance of utilizing prior views in mammography, through the review of studies where the performance of expert and less-trained radiologists was compared. Following, image registration techniques and their application to mammography are presented. Subsequently, studies that implemented temporal analysis or subtraction of temporally sequential mammograms are summarized. Finally, a description of the open access mammography datasets is provided. This comprehensive review can serve as a thorough introduction to the use of prior information in breast cancer CAD systems but also provides indicative directions to guide future applications.},
DOI = {10.3390/tomography8060241}
}






@article{ZUNAIR2021104699,
title = {Sharp U-Net: Depthwise convolutional network for biomedical image segmentation},
journal = {Computers in Biology and Medicine},
volume = {136},
pages = {104699},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104699},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521004935},
author = {Hasib Zunair and A. {Ben Hamza}},
keywords = {Sharpening filter, Semantic segmentation, Fully convolutional network, U-Net, Skip connections},
abstract = {The U-Net architecture, built upon the fully convolutional network, has proven to be effective in biomedical image segmentation. However, U-Net applies skip connections to merge semantically different low- and high-level convolutional features, resulting in not only blurred feature maps, but also over- and under-segmented target regions. To address these limitations, we propose a simple, yet effective end-to-end depthwise encoder-decoder fully convolutional network architecture, called Sharp U-Net, for binary and multi-class biomedical image segmentation. The key rationale of Sharp U-Net is that instead of applying a plain skip connection, a depthwise convolution of the encoder feature map with a sharpening kernel filter is employed prior to merging the encoder and decoder features, thereby producing a sharpened intermediate feature map of the same size as the encoder map. Using this sharpening filter layer, we are able to not only fuse semantically less dissimilar features, but also to smooth out artifacts throughout the network layers during the early stages of training. Our extensive experiments on six datasets show that the proposed Sharp U-Net model consistently outperforms or matches the recent state-of-the-art baselines in both binary and multi-class segmentation tasks, while adding no extra learnable parameters. Furthermore, Sharp U-Net outperforms baselines that have more than three times the number of learnable parameters.}
}

@misc{lundberg2017unified,
      title={A Unified Approach to Interpreting Model Predictions}, 
      author={Scott Lundberg and Su-In Lee},
      year={2017},
      eprint={1705.07874},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ribeiro2016why,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@Article{s21227741,
AUTHOR = {Jimenez-Castaño, Cristian Alfonso and Álvarez-Meza, Andrés Marino and Aguirre-Ospina, Oscar David and Cárdenas-Peña, David Augusto and Orozco-Gutiérrez, Álvaro Angel},
TITLE = {Random Fourier Features-Based Deep Learning Improvement with Class Activation Interpretability for Nerve Structure Segmentation},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {7741},
URL = {https://www.mdpi.com/1424-8220/21/22/7741},
PubMedID = {34833817},
ISSN = {1424-8220},
ABSTRACT = {Peripheral nerve blocking (PNB) is a standard procedure to support regional anesthesia. Still, correct localization of the nerve’s structure is needed to avoid adverse effects; thereby, ultrasound images are used as an aid approach. In addition, image-based automatic nerve segmentation from deep learning methods has been proposed to mitigate attenuation and speckle noise ultrasonography issues. Notwithstanding, complex architectures highlight the region of interest lacking suitable data interpretability concerning the learned features from raw instances. Here, a kernel-based deep learning enhancement is introduced for nerve structure segmentation. In a nutshell, a random Fourier features-based approach was utilized to complement three well-known semantic segmentation architectures, e.g., fully convolutional network, U-net, and ResUnet. Moreover, two ultrasound image datasets for PNB were tested. Obtained results show that our kernel-based approach provides a better generalization capability from image segmentation-based assessments on different nerve structures. Further, for data interpretability, a semantic segmentation extension of the GradCam++ for class-activation mapping was used to reveal relevant learned features separating between nerve and background. Thus, our proposal favors both straightforward (shallow) and complex architectures (deeper neural networks).},
DOI = {10.3390/s21227741}
}













@article{antonelli2022medical,
  title={The medical segmentation decathlon},
  author={Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and Kopp-Schneider, Annette and Landman, Bennett A and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M and others},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={4128},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
  

@article{li2021systematic,
  title={A systematic collection of medical image datasets for deep learning},
  author={Li, Johann and Zhu, Guangming and Hua, Cong and Feng, Mingtao and Li, Ping and Lu, Xiaoyuan and Song, Juan and Shen, Peiyi and Xu, Xu and Mei, Lin and others},
  journal={arXiv preprint arXiv:2106.12864},
  year={2021}
}

@inproceedings{Chughtai2018ComparingTE,
  title={Comparing the Effect of Aromatherapy With Essential Oils of Rosa damascena and Lavender Alone and in Combination on Severity of Pain in the First Phase of Labor in Primiparous Women},
  author={A. R. Chughtai and Maryam Navaee and Moluk Hadi Alijanvand and Fariba Yaghoubinia},
  year={2018}
}


@article{ WOS:000754419000014,
Author = {Melesse, Amsal Shiferaw and Bayable, Samuel Debas and Simegn, Getamesay
   Demelash and Ashebir, Yitayal Guadie and Ayenew, Netsanet Temesgen and
   Fetene, Melaku Bantie},
Title = {Survey on knowledge, attitude and practice of labor analgesia among
   health care providers at Debre Markos Comprehensive Specialized
   Hospital, Ethiopia 2021. A cross-sectional study},
Journal = {ANNALS OF MEDICINE AND SURGERY},
Year = {2022},
Volume = {74},
Month = {FEB},
Abstract = {Background: Childbirth is among the most painful experiences a woman has
   during their childbearing years. Despite improvement in the development
   of standards for pain assessment and treatment, labor pain is mostly
   ignored especially in low and middle-income countries.
   Objectives: To assess the knowledge, attitude, and practice of labor
   analgesia among health care providers at Debre-Markos comprehensive
   specialized hospital, Ethiopia,.
   Methods: After ethical approval was obtained from the ethical review
   board, institutional-based cross-sectional study was conducted in June
   2021, written consent was taken from each health care provider (HCP)
   before data collection, and structured self-administered questionnaires
   were used. The collected data were coded and analyzed using SPSS version
   22. Descriptive statistics were computed to determine frequencies and
   percentages finally data were presented using texts, tables, and graphs.
   The study is registered with a research unique identifying number of
   7407 found with the link address
   https://www.researchregistry.com/browse-the
   registry\#home/?view\_2\_search = 7407\&view\_2\_page = 1 and reported
   in line with STROCSS 2021.
   Results: A total of 112 health care providers have participated with
   70.5\%, 29.5\% being males and females respectively. This study was
   found that most gynecologists/obstetricians, 75\% of general
   practitioners, and more than half (57.1\%) of integrated emergency
   surgery and obstetrics (IESO) have good knowledge about labor analgesia,
   while the majority (58.3\%) of midwives found to have poor knowledge.
   Although 75\% of general practitioners and 71.4\% of IESO have a good
   attitude towards obstetric and labor pain management, only 55.6\% of
   midwives and 51.2\% of graduating medical students were found to have a
   good attitude. Among health care providers (HCP), 60.3\% of graduating
   medical students, 75\% of general practitioners, 74.1\% of IESO, and
   most of the seniors were found to have good practice of labor analgesia.
   Conclusion: There is a wide gap among health care providers in
   knowledge, attitude, and practice of labor analgesia. Training health
   care providers about safe, efficient, and affordable labor analgesia is
   crucial to improving health care.},
Publisher = {ELSEVIER SCI LTD},
Address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
Type = {Article},
Language = {English},
Affiliation = {Bayable, SD (Corresponding Author), Debre Markos Univ, Coll Med \& Hlth Sci, Dept Anaesthesia, Debre Markos, Ethiopia.
   Bayable, Samuel Debas; Simegn, Getamesay Demelash; Ashebir, Yitayal Guadie; Ayenew, Netsanet Temesgen; Fetene, Melaku Bantie, Debre Markos Univ, Coll Med \& Hlth Sci, Dept Anaesthesia, Debre Markos, Ethiopia.
   Melesse, Amsal Shiferaw, Debre Markos Univ, Coll Med, Dept Gynecol \& Obstet, Debre Markos, Ethiopia.},
DOI = {10.1016/j.amsu.2022.103306},
Article-Number = {103306},
ISSN = {2049-0801},
Keywords = {Childbirth pain; Labor analgesia; Labor pain management; Regional
   analgesia},
Keywords-Plus = {PAIN},
Research-Areas = {General \& Internal Medicine},
Web-of-Science-Categories  = {Medicine, General \& Internal},
Author-Email = {amsalshiferaw3@gmail.com
   samueldebas88@yahoo.com
   getademelash123@gmail.com
   guadyitu@gmail.com
   netsanettmsgn@gmail.com
   bmelaku088@gmail.com},
ResearcherID-Numbers = {Bayable, Samuel Debas/AFZ-5844-2022},
ORCID-Numbers = {Bayable, Samuel Debas/0000-0001-7848-1279},
Number-of-Cited-References = {23},
Times-Cited = {0},
Usage-Count-Last-180-days = {0},
Usage-Count-Since-2013 = {0},
Journal-ISO = {Ann. Med. Surg.},
Doc-Delivery-Number = {YX9MO},
Web-of-Science-Index = {Emerging Sources Citation Index (ESCI)},
Unique-ID = {WOS:000754419000014},
OA = {Green Published, gold},
DA = {2022-11-10},
}


@article{koyucu2022effects,
  title={Effects of Low Back Pain During The First Stage of Labor on Maternal Birth Satisfaction: A Cross Sectional Study},
  author={Koyucu, Refika Gen{\c{c}} and Karaca, Pelin Palas},
  journal={Ac{\i}badem {\"U}niversitesi Sa{\u{g}}l{\i}k Bilimleri Dergisi},
  volume={13},
  number={2},
  pages={247--255},
  year={2022}
}

@inproceedings{jimenez20183d,
  title={3D Probabilistic Morphable Models for Brain Tumor Segmentation},
  author={Jimenez, David A and Garc{\'\i}a, Hern{\'a}n F and {\'A}lvarez, Andres M and Orozco, {\'A}lvaro A},
  booktitle={Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 22nd Iberoamerican Congress, CIARP 2017, Valpara{\'\i}so, Chile, November 7--10, 2017, Proceedings 22},
  pages={314--322},
  year={2018},
  organization={Springer}
}




@article{teng2022survey,
  title={A survey on the interpretability of deep learning in medical diagnosis},
  author={Teng, Qiaoying and Liu, Zhe and Song, Yuqing and Han, Kai and Lu, Yang},
  journal={Multimedia Systems},
  pages={1--21},
  year={2022},
  publisher={Springer}
}

@inproceedings{gilpin2018explaining,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}


@article{lee2022comparison,
  title={Comparison of the effectiveness of local anesthesia for the digital block between single-volar subcutaneous and double-dorsal finger injections: a systematic review and meta-analysis of randomized control trials},
  author={Lee, Che-Hsiung and Lin, Mo-Han and Lin, Yu-Te and Hsu, Chung-Chen and Lin, Cheng-Hung and Chen, Shih-Heng and Huang, Ren-Wen},
  journal={Journal of Plastic Surgery and Hand Surgery},
  pages={1--14},
  year={2022},
  publisher={Taylor \& Francis}
}


@inproceedings{ghosh2022survey,
  title={A Survey on Medical Image Diagnosis Systems: Problems and Prospects},
  author={Ghosh, Krishnendu and Halder, Tamesh and Roy, Maitri and Biswas, Chandranath and Gayen, Rintu Kumar and Chakravarty, Debashish},
  booktitle={Proceedings of International Conference on Computational Intelligence, Data Science and Cloud Computing: IEM-ICDC 2021},
  pages={243--252},
  year={2022},
  organization={Springer}
}

@article{salahuddin2022transparency,
  title={Transparency of deep neural networks for medical image analysis: A review of interpretability methods},
  author={Salahuddin, Zohaib and Woodruff, Henry C and Chatterjee, Avishek and Lambin, Philippe},
  journal={Computers in biology and medicine},
  volume={140},
  pages={105111},
  year={2022},
  publisher={Elsevier}
}

@InProceedings{Ding_2019_CVPR,
author = {Ding, Henghui and Jiang, Xudong and Shuai, Bing and Liu, Ai Qun and Wang, Gang},
title = {Semantic Correlation Promoted Shape-Variant Context for Segmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}



@ARTICLE{10093993,
  author={Zhi, Yin-Cong and Ng, Yin Cheng and Dong, Xiaowen},
  journal={IEEE Transactions on Signal and Information Processing over Networks}, 
  title={Gaussian Processes on Graphs Via Spectral Kernel Learning}, 
  year={2023},
  volume={9},
  number={},
  pages={304-314},
  doi={10.1109/TSIPN.2023.3265160}}


@article{reviewmedicalnatural,
   abstract = {The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image segmentation.},
   author = {Saeid Asgari Taghanaki and Kumar Abhishek and Joseph Paul Cohen and Julien Cohen-Adad and Ghassan Hamarneh},
   doi = {10.1007/S10462-020-09854-1/FIGURES/16},
   issn = {15737462},
   issue = {1},
   journal = {Artificial Intelligence Review},
   keywords = {Deep learning,Semantic image segmentation},
   month = {1},
   note = {Argumentan el uso de arquiteturas encoder-decoder},
   pages = {137-178},
   publisher = {Springer Science and Business Media B.V.},
   title = {Deep semantic segmentation of natural and medical images: a review},
   volume = {54},
   url = {https://link-springer-com.ezproxy.unal.edu.co/article/10.1007/s10462-020-09854-1},
   year = {2021},
}


@article{alzubaidi2023survey,
  title={A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications},
  author={Alzubaidi, Laith and Bai, Jinshuai and Al-Sabaawi, Aiman and Santamar{\'\i}a, Jose and Albahri, AS and Al-dabbagh, Bashar Sami Nayyef and Fadhel, Mohammed A and Manoufali, Mohamed and Zhang, Jinglan and Al-Timemy, Ali H and others},
  journal={Journal of Big Data},
  volume={10},
  number={1},
  pages={1--82},
  year={2023},
  publisher={SpringerOpen}
}


@article{willemink2020preparing,
  title={Preparing medical imaging data for machine learning},
  author={Willemink, Martin J and Koszek, Wojciech A and Hardell, Cailin and Wu, Jie and Fleischmann, Dominik and Harvey, Hugh and Folio, Les R and Summers, Ronald M and Rubin, Daniel L and Lungren, Matthew P},
  journal={Radiology},
  volume={295},
  number={1},
  pages={4--15},
  year={2020},
  publisher={Radiological Society of North America}
}



@article{Zhang_2021,
	doi = {10.1109/tetci.2021.3100641},
  
	url = {https://doi.org/10.1109\%2Ftetci.2021.3100641},
  
	year = 2021,
	month = {oct},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {5},
  
	number = {5},
  
	pages = {726--742},
  
	author = {Yu Zhang and Peter Tino and Ales Leonardis and Ke Tang},
  
	title = {A Survey on Neural Network Interpretability},
  
	journal = {{IEEE} Transactions on Emerging Topics in Computational Intelligence}
}


@misc{singh2020explainable,
      title={Explainable deep learning models in medical image analysis}, 
      author={Amitojdeep Singh and Sourya Sengupta and Vasudevan Lakshminarayanan},
      year={2020},
      eprint={2005.13799},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



@misc{kulathunga2020effects,
      title={Effects of the Nonlinearity in Activation Functions on the Performance of Deep Learning Models}, 
      author={Nalinda Kulathunga and Nishath Rajiv Ranasinghe and Daniel Vrinceanu and Zackary Kinsman and Lei Huang and Yunjiao Wang},
      year={2020},
      eprint={2010.07359},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{SALAHUDDIN2022105111,
title = {Transparency of deep neural networks for medical image analysis: A review of interpretability methods},
journal = {Computers in Biology and Medicine},
volume = {140},
pages = {105111},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105111},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521009057},
author = {Zohaib Salahuddin and Henry C. Woodruff and Avishek Chatterjee and Philippe Lambin},
keywords = {Explainable artificial intelligence, Medical imaging, Explainability, Interpretability, Deep neural networks},
abstract = {Artificial Intelligence (AI) has emerged as a useful aid in numerous clinical applications for diagnosis and treatment decisions. Deep neural networks have shown the same or better performance than clinicians in many tasks owing to the rapid increase in the available data and computational power. In order to conform to the principles of trustworthy AI, it is essential that the AI system be transparent, robust, fair, and ensure accountability. Current deep neural solutions are referred to as black-boxes due to a lack of understanding of the specifics concerning the decision-making process. Therefore, there is a need to ensure the interpretability of deep neural networks before they can be incorporated into the routine clinical workflow. In this narrative review, we utilized systematic keyword searches and domain expertise to identify nine different types of interpretability methods that have been used for understanding deep learning models for medical image analysis applications based on the type of generated explanations and technical similarities. Furthermore, we report the progress made towards evaluating the explanations produced by various interpretability methods. Finally, we discuss limitations, provide guidelines for using interpretability methods and future directions concerning the interpretability of deep neural networks for medical imaging analysis.}
}

@article{Kumar2021SurveyOM,
  title={Survey of Machine Learning Applications of Convolutional Neural Networks to Medical Image Analysis},
  author={Dr. K. Naveen Kumar},
  journal={International Journal for Research in Applied Science and Engineering Technology},
  year={2021}
}

@article{roy2022demystifying,
  title={Demystifying supervised learning in healthcare 4.0: A new reality of transforming diagnostic medicine},
  author={Roy, Sudipta and Meena, Tanushree and Lim, Se-Jung},
  journal={Diagnostics},
  volume={12},
  number={10},
  pages={2549},
  year={2022},
  publisher={MDPI}
}

@article{dong2023adversarial,
  title={Adversarial Attack and Defense for Medical Image Analysis: Methods and Applications},
  author={Dong, Junhao and Chen, Junxi and Xie, Xiaohua and Lai, Jianhuang and Chen, Hao},
  journal={arXiv preprint arXiv:2303.14133},
  year={2023}
}

@article{Yang2021,
   abstract = {In the era of digital medicine, a vast number of medical images are produced every day. There is a great demand for intelligent equipment for adjuvant diagnosis to assist medical doctors with different disciplines. With the development of artificial intelligence, the algorithms of convolutional neural network (CNN) progressed rapidly. CNN and its extension algorithms play important roles on medical imaging classification, object detection, and semantic segmentation. While medical imaging classification has been widely reported, the object detection and semantic segmentation of imaging are rarely described. In this review article, we introduce the progression of object detection and semantic segmentation in medical imaging study. We also discuss how to accurately define the location and boundary of diseases.},
   author = {Ruixin Yang and Yingyan Yu},
   doi = {10.3389/FONC.2021.638182/BIBTEX},
   issn = {2234943X},
   journal = {Frontiers in Oncology},
   keywords = {analysis,convolutional neural network,medical images,object detection,semantic segmentation},
   pages = {573},
   publisher = {Frontiers Media S.A.},
   title = {Artificial Convolutional Neural Network in Object Detection and Semantic Segmentation for Medical Imaging Analysis},
   volume = {11},
   year = {2021},
   number={1}
}


@article{scalco2017texture,
  title={Texture analysis of medical images for radiotherapy applications},
  author={Scalco, Elisa and Rizzo, Giovanna},
  journal={The British journal of radiology},
  volume={90},
  number={1070},
  pages={20160642},
  year={2017},
  publisher={The British Institute of Radiology.}
}

@article{Jonaityt2021AnalysisOI,
  title={Analysis of Information Compression of Medical Images for Survival Models},
  author={Ieva Jonaitytė and Linas Petkevi{\v c}ius},
  journal={2021 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)},
  year={2021},
  pages={1-6}
}



@article{bougrine2022segmentation,
  title={Segmentation of Plantar Foot Thermal Images Using Prior Information},
  author={Bougrine, Asma and Harba, Rachid and Canals, Raphael and Ledee, Roger and Jabloun, Meryem and Villeneuve, Alain},
  journal={Sensors},
  volume={22},
  number={10},
  pages={3835},
  year={2022},
  publisher={MDPI}
}


@article{arteaga2021segmentation,
  title={Segmentation approaches for diabetic foot disorders},
  author={Arteaga-Marrero, Natalia and Hern{\'a}ndez, Abi{\'a}n and Villa, Enrique and Gonz{\'a}lez-P{\'e}rez, Sara and Luque, Carlos and Ruiz-Alzola, Juan},
  journal={Sensors},
  volume={21},
  number={3},
  pages={934},
  year={2021},
  publisher={MDPI}
}



@article{MALDONADO2020103187,
title = {Automatic detection of risk zones in diabetic foot soles by processing thermographic images taken in an uncontrolled environment},
journal = {Infrared Physics \& Technology},
volume = {105},
pages = {103187},
year = {2020},
issn = {1350-4495},
doi = {https://doi.org/10.1016/j.infrared.2020.103187},
url = {https://www.sciencedirect.com/science/article/pii/S1350449519308217},
author = {H. Maldonado and R. Bayareh and I.A. Torres and A. Vera and J. Gutiérrez and L. Leija},
keywords = {Diabetic foot risk detection, Thermographic images processing, Deep-learning, Transfer learning, Thermography},
abstract = {Diabetes mellitus has become a global healthcare issue with incidence levels growing exponentially each year. Diabetic foot is one of the complications related with the ailment, and if it is not attended in time, it can progressively deteriorate to a condition that necessitates foot amputation. This study aimed to establish a non-invasive monitoring system for diabetic foot. This proposed system detects and classifies temperature differences in foot sole zones as ulcerous if >2.2 °C and necrotic if <−2.2 °C, by processing thermographic images of foot soles. This is achieved without homogeneous background or room temperature control. There are reports of systems that are designed to work under controlled environments; however, their performance declines or falters altogether under uncontrolled, home environments. The system proposed in this paper combines step-by-step and end-to-end algorithms that compensate for the limitations in data and, at the same time, enhance performance. It uses deep-learning techniques to segment visible-spectrum images using a retrained Mask R-CNN model, which is adjusted with 141 images to segment foot soles. These results are used over the temperature matrix in order to isolate the foot sole temperatures. The visualisation and classification methods use step-by-step algorithms with comparisons of homologous foot sole regions, convolutions with a two-dimensional (2D) Gaussian function, filters that process and compare areas of 1.5 cm2 over each foot sole, and a thermal threshold to differentiate ulcerous from necrotic zones. The results illustrated a detection accuracy of 90\% for ulcers and 88\% for necrosis, while the labelled areas had an error of 7.05\% and 10\%, respectively. These results demonstrated that the system is capable of successfully detecting and visualising the specific temperature differences over samples under an uncontrolled environment.}
}



@inproceedings{LeCun2005TheMD,
  title={The mnist database of handwritten digits},
  author={Yann LeCun and Corinna Cortes},
  year={2005}
}

@article{rashed2022critical,
  title={Critical Analysis of the Current Medical Image-Based Processing Techniques for Automatic Disease Evaluation: Systematic Literature Review},
  author={Rashed, Baidaa Mutasher and Popescu, Nirvana},
  journal={Sensors},
  volume={22},
  number={18},
  pages={7065},
  year={2022},
  publisher={MDPI}
}

@article{Whittingham2013ImagingAI,
  title={Imaging and Imagining the Fetus – The Development of Obstetric Ultrasound},
  author={T. Whittingham},
  journal={Ultrasound},
  year={2013},
  volume={21},
  pages={235 - 235}
}


@article{gottlieb2022digital,
  title={Digital Nerve Blocks: A Comprehensive Review of Techniques},
  author={Gottlieb, Michael and Penington, Ashley and Schraft, Evelyn},
  journal={The Journal of Emergency Medicine},
  year={2022},
  publisher={Elsevier}
}


@article{ALJABRI2022311,
title = {A review on the use of deep learning for medical images segmentation},
journal = {Neurocomputing},
volume = {506},
pages = {311-335},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.07.070},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222009420},
author = {Manar Aljabri and Manal AlGhamdi},
keywords = {Convolutional neural networks, Deep learning, Medical imaging, Segmentation, Survey},
abstract = {Deep learning (DL) algorithms have rapidly become a robust tool for analyzing medical images. They have been used extensively for medical image segmentation as the first and significant components of the diagnosis and treatment pipeline. Medical image segmentation is efficiently addressed by many types of deep neural networks, such as convolutional neural networks, fully convolutional network recurrent networks, adversarial networks, and U-shaped networks. This paper reviews the major DL models and applications pertinent to medical image segmentation and summarizes over 150 contributions to the field. Brief overviews of articles are provided by application area: anatomical structures such as organs, bones, and vessels, and abnormalities such as lesions and calcification. Moreover, we discuss current challenges and suggest directions for future research.}
}


@article{Quazi2021ImageCA,
  title={Image Classification and Semantic Segmentation with Deep Learning},
  author={Saiman Quazi and Sarhan M. Musa},
  journal={2021 6th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE)},
  year={2021},
  volume={6},
  pages={1-6}
}


@article{Burel2022ImprovingDF,
  title={Improving DNN Fault Tolerance in Semantic Segmentation Applications},
  author={St{\'e}phane Burel and Adrian Evans and Lorena Anghel},
  journal={2022 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)},
  year={2022},
  pages={1-6}
}

@misc{ras2021explainable,
      title={Explainable Deep Learning: A Field Guide for the Uninitiated}, 
      author={Gabrielle Ras and Ning Xie and Marcel van Gerven and Derek Doran},
      year={2021},
      eprint={2004.14545},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mohankumar2020transparent,
      title={Towards Transparent and Explainable Attention Models}, 
      author={Akash Kumar Mohankumar and Preksha Nema and Sharan Narasimhan and Mitesh M. Khapra and Balaji Vasan Srinivasan and Balaraman Ravindran},
      year={2020},
      eprint={2004.14243},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{speith2022review,
  title={A review of taxonomies of explainable artificial intelligence (XAI) methods},
  author={Speith, Timo},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2239--2250},
  year={2022}
}

@INPROCEEDINGS{9879843,
  author={Sarkar, Anirban and Vijaykeerthy, Deepak and Sarkar, Anindya and Balasubramanian, Vineeth N},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Framework for Learning Ante-hoc Explainable Models via Concepts}, 
  year={2022},
  volume={},
  number={},
  pages={10276-10285},
  doi={10.1109/CVPR52688.2022.01004}}


@article{https://doi.org/10.1002/widm.1312,
author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
title = {Causability and explainability of artificial intelligence in medicine},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {9},
number = {4},
pages = {e1312},
keywords = {artificial intelligence, causability, explainability, explainable AI, histopathology, medicine},
doi = {https://doi.org/10.1002/widm.1312},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1312},
abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge > Human Centricity and User Interaction},
year = {2019}
}




@article{linardatos2020explainable,
  title={Explainable ai: A review of machine learning interpretability methods},
  author={Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  journal={Entropy},
  volume={23},
  number={1},
  pages={18},
  year={2020},
  publisher={MDPI}
}

@article{amann2020explainability,
  title={Explainability for artificial intelligence in healthcare: a multidisciplinary perspective},
  author={Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I and Precise4Q Consortium},
  journal={BMC medical informatics and decision making},
  volume={20},
  pages={1--9},
  year={2020},
  publisher={Springer}
}

@article{MARKUS2021103655,
title = {The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies},
journal = {Journal of Biomedical Informatics},
volume = {113},
pages = {103655},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103655},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
author = {Aniek F. Markus and Jan A. Kors and Peter R. Rijnbeek},
keywords = {Explainable artificial intelligence, Trustworthy artificial intelligence, Interpretability, Explainable modelling, Post-hoc explanation},
abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).}
}




@article{kolyshkina2021interpretability,
  title={Interpretability of machine learning solutions in public healthcare: The CRISP-ML approach},
  author={Kolyshkina, Inna and Simoff, Simeon},
  journal={Frontiers in big Data},
  volume={4},
  pages={660206},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{xu2023interpretability,
  title={Interpretability of Clinical Decision Support Systems Based on Artificial Intelligence from Technological and Medical Perspective: A Systematic Review},
  author={Xu, Qian and Xie, Wenzhao and Liao, Bolin and Hu, Chao and Qin, Lu and Yang, Zhengzijin and Xiong, Huan and Lyu, Yi and Zhou, Yue and Luo, Aijing and others},
  journal={Journal of Healthcare Engineering},
  volume={2023},
  year={2023},
  publisher={Hindawi}
}




@misc{zhou2020unet,
      title={UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation}, 
      author={Zongwei Zhou and Md Mahfuzur Rahman Siddiquee and Nima Tajbakhsh and Jianming Liang},
      year={2020},
      eprint={1912.05074},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}


@misc{bennetot2022practical,
      title={A Practical guide on Explainable AI Techniques applied on Biomedical use case applications}, 
      author={Adrien Bennetot and Ivan Donadello and Ayoub El Qadi and Mauro Dragoni and Thomas Frossard and Benedikt Wagner and Anna Saranti and Silvia Tulli and Maria Trocan and Raja Chatila and Andreas Holzinger and Artur d'Avila Garcez and Natalia Díaz-Rodríguez},
      year={2022},
      eprint={2111.14260},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{BRON2015562,
title = {Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: The CADDementia challenge},
journal = {NeuroImage},
volume = {111},
pages = {562-579},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915000737},
author = {Esther E. Bron and Marion Smits and Wiesje M. {van der Flier} and Hugo Vrenken and Frederik Barkhof and Philip Scheltens and Janne M. Papma and Rebecca M.E. Steketee and Carolina {Méndez Orellana} and Rozanna Meijboom and Madalena Pinto and Joana R. Meireles and Carolina Garrett and António J. Bastos-Leite and Ahmed Abdulkadir and Olaf Ronneberger and Nicola Amoroso and Roberto Bellotti and David Cárdenas-Peña and Andrés M. Álvarez-Meza and Chester V. Dolph and Khan M. Iftekharuddin and Simon F. Eskildsen and Pierrick Coupé and Vladimir S. Fonov and Katja Franke and Christian Gaser and Christian Ledig and Ricardo Guerrero and Tong Tong and Katherine R. Gray and Elaheh Moradi and Jussi Tohka and Alexandre Routier and Stanley Durrleman and Alessia Sarica and Giuseppe {Di Fatta} and Francesco Sensi and Andrea Chincarini and Garry M. Smith and Zhivko V. Stoyanov and Lauge Sørensen and Mads Nielsen and Sabina Tangaro and Paolo Inglese and Christian Wachinger and Martin Reuter and John C. {van Swieten} and Wiro J. Niessen and Stefan Klein},
keywords = {Alzheimer's disease, Challenge, Classification, Computer-aided diagnosis, Mild cognitive impairment, Structural MRI},
abstract = {Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment and healthy controls. The diagnosis based on clinical criteria was used as reference standard, as it was the best available reference despite its known limitations. For evaluation, a previously unseen test set was used consisting of 354 T1-weighted MRI scans with the diagnoses blinded. Fifteen research teams participated with a total of 29 algorithms. The algorithms were trained on a small training set (n=30) and optionally on data from other sources (e.g., the Alzheimer's Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of aging). The best performing algorithm yielded an accuracy of 63.0\% and an area under the receiver-operating-characteristic curve (AUC) of 78.8\%. In general, the best performances were achieved using feature extraction based on voxel-based morphometry or a combination of features that included volume, cortical thickness, shape and intensity. The challenge is open for new submissions via the web-based framework: http://caddementia.grand-challenge.org.}
}

@article{cavazos2015maternal,
  title={Maternal age and risk of labor and delivery complications},
  author={Cavazos-Rehg, Patricia A and Krauss, Melissa J and Spitznagel, Edward L and Bommarito, Kerry and Madden, Tessa and Olsen, Margaret A and Subramaniam, Harini and Peipert, Jeffrey F and Bierut, Laura Jean},
  journal={Maternal and child health journal},
  volume={19},
  pages={1202--1211},
  year={2015},
  publisher={Springer}
}

@article{parise2021childbirth,
  title={Childbirth pain and post-partum depression: Does labor epidural analgesia decrease this risk?},
  author={Parise, Daniele C and Gilman, Caitlin and Petrilli, Matthew A and Malaspina, Dolores},
  journal={Journal of Pain Research},
  pages={1925--1933},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{10.1093/pm/pnaa380,
    author = {Whitburn, Laura Yvette and Jones, Lester Edmund},
    title = "{Looking for Meaning in Labour Pain: Are Current Pain Measurement Tools Adequate?}",
    journal = {Pain Medicine},
    volume = {22},
    number = {5},
    pages = {1023-1028},
    year = {2020},
    month = {11},
    issn = {1526-4637},
    doi = {10.1093/pm/pnaa380},
    url = {https://doi.org/10.1093/pm/pnaa380},
    eprint = {https://academic.oup.com/painmedicine/article-pdf/22/5/1023/38095497/pnaa380.pdf},
}


@article{smith2018relaxation,
  title={Relaxation techniques for pain management in labour},
  author={Smith, Caroline A and Levett, Kate M and Collins, Carmel T and Armour, Mike and Dahlen, Hannah G and Suganuma, Machiko},
  journal={Cochrane Database of Systematic Reviews},
  number={3},
  year={2018},
  publisher={John Wiley \& Sons, Ltd}
}


@article {Joensuue061186,
	author = {Johanna Joensuu and Hannu Saarij{\"a}rvi and Hanna Rouhe and Mika Gissler and Veli-Matti Ulander and Seppo Heinonen and Paulus Torkki and Tomi Mikkola},
	title = {Maternal childbirth experience and pain relief methods: a retrospective 7-year cohort study of 85 488 parturients in Finland},
	volume = {12},
	number = {5},
	elocation-id = {e061186},
	year = {2022},
	doi = {10.1136/bmjopen-2022-061186},
	publisher = {British Medical Journal Publishing Group},
	abstract = {Objectives The aim of this study was to analyse the relation between the used labour pain relief and childbirth experience measured by Visual Analogue Scale (VAS).Design A retrospective cohort study.Setting Childbirth in five Helsinki University Hospital delivery units from 2012 to 2018.Primary outcome measure Childbirth experience measured by VAS and classified in three groups (negative VAS=1{\textendash}5, positive VAS=6{\textendash}8 and highly positive=9{\textendash}10).Results The use of epidural or non-epidural compared with non-medical pain relief methods decreased the likelihood to experience highly positive childbirth for primiparous (adjusted OR (aOR)EPIDURAL=0.64, 95\% CI 0.57 to 0.73; and aORNON-EPIDURAL=0.76, 95\% CI 0.66 to 0.87) and multiparous (aOREPIDURAL=0.90, 95\% CI 0.84 to 0.97 and aORNON-EPIDURAL=0.80, 95\% CI 0.74 to 0.86) parturients. The effects of epidural differed between primiparas and multiparas. In multiparas epidural was associated with decreased odds for experiencing negative childbirth compared with the non-medical group (aOR=0.70, 95\% CI 0.57 to 0.87), while the effect of epidural was considered insignificant in primiparas (aOR=1.28, 95\% CI 0.93 to 1.77).Conclusion While the use of medical{\textemdash}epidural and non-epidural{\textemdash}pain relief methods were not associated with odds for experiencing negative childbirth in primiparas, using epidural helps to avoid negative experience in multiparas. However, the odds for experiencing highly positive childbirth were decreased if the parturients used any medical pain relief for both primiparas and multiparas. Consequently, the effect of pain relief on the childbirth experience is strongly confounded by indication. Thus, the use of pain relief per se plays a limited role in the complex formation of the overall childbirth experience.No data are available. The data was granted to use in this study and is not allowed to reuse without other permission.},
	issn = {2044-6055},
	URL = {https://bmjopen.bmj.com/content/12/5/e061186},
	eprint = {https://bmjopen.bmj.com/content/12/5/e061186.full.pdf},
	journal = {BMJ Open}
}




@misc{who,
  author       = {{World Health Organization}},
  title        = {Maternal health},
  url = {Available online at \url{https://www.who.int/health-topics/maternal-health#tab=tab_1}},
  year         = {2023},
  note         = {Accessed on May 4, 2023},
}



 @misc{minMaterno, title={Salud Materna \&nbsp; // }, url={https://www.minsalud.gov.co/salud/publica/ssr/Paginas/salud-materna.aspx}, journal={Inicio}, author={{Ministerio de Salud y Protección Social de Colombia}},
 year={2023}} 


@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@article{azad2023advances,
  title={Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review},
  author={Azad, Reza and Kazerouni, Amirhossein and Heidari, Moein and Aghdam, Ehsan Khodapanah and Molaei, Amirali and Jia, Yiwei and Jose, Abin and Roy, Rijo and Merhof, Dorit},
  journal={arXiv preprint arXiv:2301.03505},
  year={2023}
}

@article{omer2022impact,
  title={Impact of COVID-19 status on patients receiving neuraxial analgesia during labor: A national retrospective-controlled study},
  author={Omer Ibrahim Abdalla, Eynas and Nahid, Seema and Shastham Valappil, Sikha and Gudavalli, Srinivas and Sellami, Soumaya and Korichi, Noureddine and Ahmad, Shamsa and Vicente Canizares Cespedes, Victor and Gopalakrishnan, Santhosh},
  journal={Qatar Medical Journal},
  volume={2022},
  number={3},
  pages={30},
  year={2022},
  publisher={HBKU Press Qatar}
}


@article{alebiosu2023improving,
  title={Improving tuberculosis severity assessment in computed tomography images using novel DAvoU-Net segmentation and deep learning framework},
  author={Alebiosu, David Olayemi and Dharmaratne, Anuja and Lim, Chern Hong},
  journal={Expert Systems with Applications},
  volume={213},
  pages={119287},
  year={2023},
  publisher={Elsevier}
}


@article{yildirim2023development,
  title={Development of an Artificial Intelligence Method to Detect COVID-19 Pneumonia in Computed Tomography Images.},
  author={Y{\i}ld{\i}r{\i}m, G{\"u}l{\c{s}}ah and Karaka{\c{s}}, Hakk{\i} Muammer and {\"O}zkaya, Ya{\c{s}}ar Alper and {\c{S}}ener, Emre and F{\i}nd{\i}k, {\"O}zge and Pulat, G{\"u}lhan Naz},
  journal={Istanbul Medical Journal},
  volume={24},
  number={1},
  year={2023}
}

@inproceedings{jahwar2022segmentation,
  title={Segmentation and classification for breast cancer ultrasound images using deep learning techniques: A review},
  author={Jahwar, Alan Fuad and Abdulazeez, Adnan Mohsin},
  booktitle={2022 IEEE 18th International Colloquium on Signal Processing \& Applications (CSPA)},
  pages={225--230},
  year={2022},
  organization={IEEE}
}

@article{fazilov2022mammography,
  title={Mammography image segmentation in breast cancer identification using the otsu method},
  author={Fazilov, Sh Kh and Yusupov, OR and Abdiyeva, Kh S},
  journal={Web of Scientist: International Scientific Research Journal},
  volume={3},
  number={8},
  pages={196--205},
  year={2022}
}


@Article{diagnostics12081812,
AUTHOR = {Altameem, Ayman and Mahanty, Chandrakanta and Poonia, Ramesh Chandra and Saudagar, Abdul Khader Jilani and Kumar, Raghvendra},
TITLE = {Breast Cancer Detection in Mammography Images Using Deep Convolutional Neural Networks and Fuzzy Ensemble Modeling Techniques},
JOURNAL = {Diagnostics},
VOLUME = {12},
YEAR = {2022},
NUMBER = {8},
ARTICLE-NUMBER = {1812},
URL = {https://www.mdpi.com/2075-4418/12/8/1812},
PubMedID = {36010164},
ISSN = {2075-4418},
ABSTRACT = {Breast cancer has evolved as the most lethal illness impacting women all over the globe. Breast cancer may be detected early, which reduces mortality and increases the chances of a full recovery. Researchers all around the world are working on breast cancer screening tools based on medical imaging. Deep learning approaches have piqued the attention of many in the medical imaging field due to their rapid growth. In this research, mammography pictures were utilized to detect breast cancer. We have used four mammography imaging datasets with a similar number of 1145 normal, benign, and malignant pictures using various deep CNN (Inception V4, ResNet-164, VGG-11, and DenseNet121) models as base classifiers. The proposed technique employs an ensemble approach in which the Gompertz function is used to build fuzzy rankings of the base classification techniques, and the decision scores of the base models are adaptively combined to construct final predictions. The proposed fuzzy ensemble techniques outperform each individual transfer learning methodology as well as multiple advanced ensemble strategies (Weighted Average, Sugeno Integral) with reference to prediction and accuracy. The suggested Inception V4 ensemble model with fuzzy rank based Gompertz function has a 99.32\% accuracy rate. We believe that the suggested approach will be of tremendous value to healthcare practitioners in identifying breast cancer patients early on, perhaps leading to an immediate diagnosis.},
DOI = {10.3390/diagnostics12081812}
}






@article{LIU2020116459,
title = {A multi-model deep convolutional neural network for automatic hippocampus segmentation and classification in Alzheimer’s disease},
journal = {NeuroImage},
volume = {208},
pages = {116459},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116459},
url = {https://www.sciencedirect.com/science/article/pii/S105381191931050X},
author = {Manhua Liu and Fan Li and Hao Yan and Kundong Wang and Yixin Ma and Li Shen and Mingqing Xu},
keywords = {Alzheimer’s disease, Hippocampus, Magnetic resonance imaging, Convolutional neural network, Image classification},
abstract = {Alzheimer’s disease (AD) is a progressive and irreversible brain degenerative disorder. Mild cognitive impairment (MCI) is a clinical precursor of AD. Although some treatments can delay its progression, no effective cures are available for AD. Accurate early-stage diagnosis of AD is vital for the prevention and intervention of the disease progression. Hippocampus is one of the first affected brain regions in AD. To help AD diagnosis, the shape and volume of the hippocampus are often measured using structural magnetic resonance imaging (MRI). However, these features encode limited information and may suffer from segmentation errors. Additionally, the extraction of these features is independent of the classification model, which could result in sub-optimal performance. In this study, we propose a multi-model deep learning framework based on convolutional neural network (CNN) for joint automatic hippocampal segmentation and AD classification using structural MRI data. Firstly, a multi-task deep CNN model is constructed for jointly learning hippocampal segmentation and disease classification. Then, we construct a 3D Densely Connected Convolutional Networks (3D DenseNet) to learn features of the 3D patches extracted based on the hippocampal segmentation results for the classification task. Finally, the learned features from the multi-task CNN and DenseNet models are combined to classify disease status. Our method is evaluated on the baseline T1-weighted structural MRI data collected from 97 AD, 233 MCI, 119 Normal Control (NC) subjects in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database. The proposed method achieves a dice similarity coefficient of 87.0\% for hippocampal segmentation. In addition, the proposed method achieves an accuracy of 88.9\%, an AUC (area under the ROC curve) of 92.5\% for classifying AD vs. NC subjects, and an accuracy of 76.2\% and an AUC of 77.5\% for classifying MCI vs. NC subjects. Our empirical study also demonstrates that the proposed multi-model method outperforms the single-model methods and several other competing methods.}
}

@article{CARMO2021e06226,
title = {Hippocampus segmentation on epilepsy and Alzheimer's disease studies with multiple convolutional neural networks},
journal = {Heliyon},
volume = {7},
number = {2},
pages = {e06226},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06226},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021003315},
author = {Diedre Carmo and Bruna Silva and Clarissa Yasuda and Letícia Rittner and Roberto Lotufo},
keywords = {Deep learning, Hippocampus segmentation, Convolutional neural networks, Alzheimer's disease, Epilepsy},
abstract = {Background: Hippocampus segmentation on magnetic resonance imaging is of key importance for the diagnosis, treatment decision and investigation of neuropsychiatric disorders. Automatic segmentation is an active research field, with many recent models using deep learning. Most current state-of-the art hippocampus segmentation methods train their methods on healthy or Alzheimer's disease patients from public datasets. This raises the question whether these methods are capable of recognizing the hippocampus on a different domain, that of epilepsy patients with hippocampus resection. New Method: In this paper we present a state-of-the-art, open source, ready-to-use, deep learning based hippocampus segmentation method. It uses an extended 2D multi-orientation approach, with automatic pre-processing and orientation alignment. The methodology was developed and validated using HarP, a public Alzheimer's disease hippocampus segmentation dataset. Results and Comparisons: We test this methodology alongside other recent deep learning methods, in two domains: The HarP test set and an in-house epilepsy dataset, containing hippocampus resections, named HCUnicamp. We show that our method, while trained only in HarP, surpasses others from the literature in both the HarP test set and HCUnicamp in Dice. Additionally, Results from training and testing in HCUnicamp volumes are also reported separately, alongside comparisons between training and testing in epilepsy and Alzheimer's data and vice versa. Conclusion: Although current state-of-the-art methods, including our own, achieve upwards of 0.9 Dice in HarP, all tested methods, including our own, produced false positives in HCUnicamp resection regions, showing that there is still room for improvement for hippocampus segmentation methods when resection is involved.}
}



@article {30877605,
	Title = {Fast and Precise Hippocampus Segmentation Through Deep Convolutional Neural Network Ensembles and Transfer Learning},
	Author = {Ataloglou, Dimitrios and Dimou, Anastasios and Zarpalas, Dimitrios and Daras, Petros},
	DOI = {10.1007/s12021-019-09417-y},
	Number = {4},
	Volume = {17},
	Month = {October},
	Year = {2019},
	Journal = {Neuroinformatics},
	ISSN = {1539-2791},
	Pages = {563—582},
	Abstract = {Automatic segmentation of the hippocampus from 3D magnetic resonance imaging mostly relied on multi-atlas registration methods. In this work, we exploit recent advances in deep learning to design and implement a fully automatic segmentation method, offering both superior accuracy and fast result. The proposed method is based on deep Convolutional Neural Networks (CNNs) and incorporates distinct segmentation and error correction steps. Segmentation masks are produced by an ensemble of three independent models, operating with orthogonal slices of the input volume, while erroneous labels are subsequently corrected by a combination of Replace and Refine networks. We explore different training approaches and demonstrate how, in CNN-based segmentation, multiple datasets can be effectively combined through transfer learning techniques, allowing for improved segmentation quality. The proposed method was evaluated using two different public datasets and compared favorably to existing methodologies. In the EADC-ADNI HarP dataset, the correspondence between the method's output and the available ground truth manual tracings yielded a mean Dice value of 0.9015, while the required segmentation time for an entire MRI volume was 14.8 seconds. In the MICCAI dataset, the mean Dice value increased to 0.8835 through transfer learning from the larger EADC-ADNI HarP dataset.},
	URL = {https://doi.org/10.1007/s12021-019-09417-y},
}

@article{WU2021107904,
title = {Brain segmentation based on multi-atlas and diffeomorphism guided 3D fully convolutional network ensembles},
journal = {Pattern Recognition},
volume = {115},
pages = {107904},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107904},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000911},
author = {Jiong Wu and Xiaoying Tang},
keywords = {Brain segmentation, Fully convolutional network, Multi-atlas, Diffeomorphism, Adaptive-size patches, Ensemble model},
abstract = {In this study, we proposed and validated a multi-atlas and diffeomorphism guided 3D fully convolutional network (FCN) ensemble model (M-FCN) for segmenting brain anatomical regions of interest (ROIs) from structural magnetic resonance images (MRIs). A novel multi-atlas and diffeomorphism based encoding block and ROI patches with adaptive sizes were used. In the multi-atlas and diffeomorphism based encoding block, both MRI intensity profiles and expert priors from deformed atlases were encoded and fed to the proposed FCN. Utilizing patches with adaptive sizes enabled more efficient network training and testing. To incorporate both local and global contextual information of a specific ROI, we employed a long skip connection between the layer of the encoding block and the layer of the encoding-decoding block. To relieve over-fitting of the proposed FCN model on the training data, we adopted an ensemble strategy in the learning procedure. Systematic evaluations were performed on two brain MRI datasets, aiming respectively at segmenting 14 subcortical and ventricular structures and 54 whole-brain ROIs. Compared with two state-of-the-art segmentation methods including a multi-atlas based segmentation method and an existing 3D FCN segmentation model, the proposed method exhibited superior segmentation performance.}
}



@article{Mubashar_2022,
	doi = {10.1007/s00521-022-07419-7},
  
	url = {https://doi.org/10.1007\%2Fs00521-022-07419-7},
  
	year = 2022,
	month = {jun},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {34},
  
	number = {20},
  
	pages = {17723--17739},
  
	author = {Mehreen Mubashar and Hazrat Ali and Christer Grönlund and Shoaib Azmat},
  
	title = {R2U++: a multiscale recurrent residual U-Net with dense skip connections for medical image segmentation},
  
	journal = {Neural Computing and Applications}
}
  
@misc{nair2021pushing,
      title={Pushing the Limits of Capsule Networks}, 
      author={Prem Nair and Rohan Doshi and Stefan Keselj},
      year={2021},
      eprint={2103.08074},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}




@inproceedings{NEURIPS2020_47fd3c87,
 author = {De Sousa Ribeiro, Fabio and Leontidis, Georgios and Kollias, Stefanos},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6490--6502},
 publisher = {Curran Associates, Inc.},
 title = {Introducing Routing Uncertainty in Capsule Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/47fd3c87f42f55d4b233417d49c34783-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{SemanticMedicalImage18,
  author    = {Lena Maier{-}Hein and
               Matthias Eisenmann and
               Annika Reinke and
               Sinan Onogur and
               Marko Stankovic and
               Patrick Scholz and
               Tal Arbel and
               Hrvoje Bogunovic and
               Andrew P. Bradley and
               Aaron Carass and
               Carolin Feldmann and
               Alejandro F. Frangi and
               Peter M. Full and
               Bram van Ginneken and
               Allan Hanbury and
               Katrin Honauer and
               Michal Kozubek and
               Bennett A. Landman and
               Keno M{\"{a}}rz and
               Oskar Maier and
               Klaus H. Maier{-}Hein and
               Bjoern H. Menze and
               Henning M{\"{u}}ller and
               Peter F. Neher and
               Wiro J. Niessen and
               Nasir M. Rajpoot and
               Gregory C. Sharp and
               Korsuk Sirinukunwattana and
               Stefanie Speidel and
               Christian Stock and
               Danail Stoyanov and
               Abdel Aziz Taha and
               Fons van der Sommen and
               Ching{-}Wei Wang and
               Marc{-}Andr{\'{e}} Weber and
               Guoyan Zheng and
               Pierre Jannin and
               Annette Kopp{-}Schneider},
  title     = {Is the winner really the best? {A} critical analysis of common research
               practice in biomedical image analysis competitions},
  journal   = {CoRR},
  volume    = {abs/1806.02051},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.02051},
  archivePrefix = {arXiv},
  eprint    = {1806.02051},
  timestamp = {Thu, 12 Nov 2020 09:13:22 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-02051.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{bengio2017scaling,
  title={Scaling Learning Algorithms Towards AI To Appear in “Large-Scale Kernel Machines”},
  author={Bengio, Y and Lecun, Y},
  journal={George Mason University: Fairfax, VA, USA},
  year={2017}
}



@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}






@article{MO2022626,
title = {Review the state-of-the-art technologies of semantic segmentation based on deep learning},
journal = {Neurocomputing},
volume = {493},
pages = {626-646},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222000054},
author = {Yujian Mo and Yan Wu and Xinneng Yang and Feilin Liu and Yujun Liao},
keywords = {Deep learning, Convolutional neural networks, Semantic segmentation, Real-time, Domain adaptation, Multi-modal fusion, Weakly-supervised},
abstract = {The goal of semantic segmentation is to segment the input image according to semantic information and predict the semantic category of each pixel from a given label set. With the gradual intellectualization of modern life, more and more applications need to infer relevant semantic information from images for subsequent processing, such as augmented reality, autonomous driving, video surveillance, etc. This paper reviews the state-of-the-art technologies of semantic segmentation based on deep learning. Because semantic segmentation requires a large number of pixel-level annotations, in order to reduce the fine-grained requirements of annotation and reduce the economic and time cost of manual annotation, this paper studies the works on weakly-supervised semantic segmentation. In order to enhance the generalization ability and robustness of the segmentation model, this paper investigates the works on domain adaptation in semantic segmentation. Many types of sensors are usually equipped in some practical applications, such as autonomous driving and medical image analysis. In order to mine the association between multi-modal data and improve the accuracy of the segmentation model, this paper investigates the works based on multi-modal data fusion semantic segmentation. The real-time performance of the model needs to be considered in practical application. This paper analyzes the key factors affecting the real-time performance of the segmentation model and investigates the works on real-time semantic segmentation. Finally, this paper summarizes the challenges and promising research directions of semantic segmentation tasks based on deep learning.}
}

@misc{cakir2022semantic,
      title={Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability}, 
      author={Senay Cakir and Marcel Gauß and Kai Häppeler and Yassine Ounajjar and Fabian Heinle and Reiner Marchthaler},
      year={2022},
      eprint={2207.12939},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{QURESHI2023316,
title = {Medical image segmentation using deep semantic-based methods: A review of techniques, applications and emerging trends},
journal = {Information Fusion},
volume = {90},
pages = {316-352},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001695},
author = {Imran Qureshi and Junhua Yan and Qaisar Abbas and Kashif Shaheed and Awais Bin Riaz and Abdul Wahid and Muhammad Waseem Jan Khan and Piotr Szczuko},
keywords = {Deep learning, Medical imaging, Optimization techniques, Transfer learning, Semantic segmentation},
abstract = {Semantic-based segmentation (Semseg) methods play an essential part in medical imaging analysis to improve the diagnostic process. In Semseg technique, every pixel of an image is classified into an instance, where each class is corresponded by an instance. In particular, the semantic segmentation can be used by many medical experts in the domain of radiology, ophthalmologists, dermatologist, and image-guided radiotherapy. The authors present perspectives on the development of an architectural, and operational mechanism of each machine learning-based semantic segmentation approach with merits and demerits. In this regard, researchers have proposed different Semseg methods and examined their performance in a variety of applications such as medical image analysis (e.g., medical image classification and segmentation). A review of recent advances in Semseg techniques are presented in this paper by applying computational image processing and machine learning methods. This article is further presented a comprehensive investigation on how different architectures are helpful for medical image segmentation. Finally, advantages, open challenges, and possible future directions are elaborated in the discussion part, beneficial to the research community to understand the significance of the available medical imaging segmentation technology based on Semseg and thus deliver robust segmentation solutions.}
}


@Article{s23020895,
AUTHOR = {Tsai, Jichiang and Chang, Che-Cheng and Li, Tzu},
TITLE = {Autonomous Driving Control Based on the Technique of Semantic Segmentation},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {2},
ARTICLE-NUMBER = {895},
URL = {https://www.mdpi.com/1424-8220/23/2/895},
PubMedID = {36679688},
ISSN = {1424-8220},
ABSTRACT = {Advanced Driver Assistance Systems (ADAS) are only applied to relatively simple scenarios, such as highways. If there is an emergency while driving, the driver should take control of the car to deal properly with the situation at any time. Obviously, this incurs the uncertainty of safety. Recently, in the literature, several studies have been proposed for the above-mentioned issue via Artificial Intelligence (AI). The achievement is exactly the aim that we look forward to, i.e., the autonomous vehicle. In this paper, we realize the autonomous driving control via Deep Reinforcement Learning (DRL) based on the CARLA (Car Learning to Act) simulator. Specifically, we use the ordinary Red-Green-Blue (RGB) camera and semantic segmentation camera to observe the view in front of the vehicle while driving. Then, the captured information is utilized as the input for different DRL models so as to evaluate the performance, where the DRL models include DDPG (Deep Deterministic Policy Gradient) and RDPG (Recurrent Deterministic Policy Gradient). Moreover, we also design an appropriate reward mechanism for these DRL models to realize efficient autonomous driving control. According to the results, only the RDPG strategies can finish the driving mission with the scenario that does not appear/include in the training scenario, and with the help of the semantic segmentation camera, the RDPG control strategy can further improve its efficiency.},
DOI = {10.3390/s23020895}
}


@ARTICLE{9356353,
  author={Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Image Segmentation Using Deep Learning: A Survey}, 
  year={2022},
  volume={44},
  number={7},
  pages={3523-3542},
  doi={10.1109/TPAMI.2021.3059968}}

@article{WIELAND2023113452,
title = {Semantic segmentation of water bodies in very high-resolution satellite and aerial images},
journal = {Remote Sensing of Environment},
volume = {287},
pages = {113452},
year = {2023},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2023.113452},
url = {https://www.sciencedirect.com/science/article/pii/S0034425723000032},
author = {Marc Wieland and Sandro Martinis and Ralph Kiefl and Veronika Gstaiger},
keywords = {Convolutional neural networks, Semantic segmentation, Water, Emergency response, Rapid mapping},
abstract = {This study evaluates the performance of convolutional neural networks for semantic segmentation of water bodies in very high-resolution satellite and aerial images from multiple sensors with particular focus on flood emergency response applications. Different model architectures (U-Net and DeepLab-V3+) are combined with encoder backbones (MobileNet-V3, ResNet-50 and EfficientNet-B4) and tested for their ability to delineate inundated areas under varying environmental conditions and data availability scenarios. An unprecedented reference dataset of 1120 globally sampled images with quality checked binary water masks is introduced and used to train, validate and test the models for water body segmentation. Furthermore, independent test datasets are developed to test the generalization ability of the trained models across regions, sensors (IKONOS, GeoEye-1, WorldView-2, WorldView-3 and four different airborne camera systems) and tasks (normal water and flood water segmentation). Results indicate that across all tested scenarios a U-Net model with Mobilenet-V3 backbone pre-trained on ImageNet performs best. While using R-G-B image bands performs well, adding the near infrared band (if available) slightly improves prediction results. Similarly, adding slope information from an independent digital elevation model increases accuracies. Train-time augmentation and contrast enhancement could improve transferability across sensors and in particular between satellite and aerial images. Moreover, adding noisy training data from freely available online resources could further improve performance with minimal annotation effort.}
}

@article{lilay2023semantic,
  title={Semantic segmentation model for land cover classification from satellite images in Gambella National Park, Ethiopia},
  author={Lilay, Mulugeta Yikuno and Taye, Gizatie Desalegn},
  journal={SN Applied Sciences},
  volume={5},
  number={3},
  pages={76},
  year={2023},
  publisher={Springer}
}

@article{SINGH2021105986,
title = {Disease and pest infection detection in coconut tree through deep learning techniques},
journal = {Computers and Electronics in Agriculture},
volume = {182},
pages = {105986},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.105986},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921000041},
author = {Piyush Singh and Abhishek Verma and John Sahaya Rani Alex},
keywords = {Coconut tree, Deep learning, InceptionResNetV2, MobileNet, Transfer learning},
abstract = {The coconut palm plantation industry relies heavily on expert advice to identify and treat infections. Computer vision in deep learning technology opened up an avenue in the agriculture domain to find a solution. This study focuses on the development of an end-to-end framework to detect stem bleeding disease, leaf blight disease, and pest infection by Red palm weevil in coconut trees by applying image processing and deep learning technology. A set of hand-collected images of healthy and unhealthy coconut tree images were segmented by employing popular segmentation algorithms to easily locate the abnormal boundaries. The custom-designed deep 2D-Convolutional Neural Network (CNN) is trained to predict diseases and pest infections. Also, the state of the art Keras pre-trained CNN models VGG16, VGG19, InceptionV3, DenseNet201, MobileNet, Xception, InceptionResNetV2, and NASNetMobile were fine-tuned to classify the images either as infected or as healthy through the inductive transfer learning method. The empirical study ascertains that k-means clustering segmentation was more effective than the Thresholding and Watershed segmentation methods. Furthermore, InceptionResNetV2 and MobileNet obtained a classification accuracy of 81.48\% and 82.10\%, respectively, and Cohen’s Kappa values of 0.77 and 0.74, respectively. The hand-designed CNN model achieved 96.94\% validation accuracy with a Kappa value of 0.91. The MobileNet model and customized 2D-CNN model were deployed in the web application through the micro-web framework Flask to automatically detect the coconut tree disease or pest infection.}
}

@article{ZOU2021106242,
title = {A modified U-Net with a specific data argumentation method for semantic segmentation of weed images in the field},
journal = {Computers and Electronics in Agriculture},
volume = {187},
pages = {106242},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106242},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921002593},
author = {Kunlin Zou and Xin Chen and Yonglin Wang and Chunlong Zhang and Fan Zhang},
keywords = {Weed segmentation, Precise weeding, Semantic segmentation, Deep learning, U-Net},
abstract = {Weeds are harmful to crop yield. The segmentation of weeds in images is of great significance for precise weeding and reducing herbicide pollution. However, in the field environment, crops and weeds are similar, so it is difficult to accurately segment weed from complex field images. In this paper, an algorithm based on deep learning was proposed to segment weeds from images. This algorithm can segment weeds from the soil and crops in images. This semantic segmentation algorithm was developed with a simplified U-net. Due to the difficulty of image labeling for the semantic segmentation of weeds, an image augmentation method was proposed. The semantic segmentation network was trained by a two-stage training method composed of pre-training and fine-tuning. After training, the intersection over union (IoU) of this method was 92.91\% and the average segmentation time of a single image (ST) was 51.71 ms. The results demonstrated that the modified U-Net was able to effectively segment weeds from images with a significant amount of other plants. The weed-targeted image segmentation method proposed in this paper can accurately segment weeds in complex field environments. It has a relatively wide range of applicability.}
}

@ARTICLE{9395478,
  author={Anand, Tanmay and Sinha, Soumendu and Mandal, Murari and Chamola, Vinay and Yu, Fei Richard},
  journal={IEEE Sensors Journal}, 
  title={AgriSegNet: Deep Aerial Semantic Segmentation Framework for IoT-Assisted Precision Agriculture}, 
  year={2021},
  volume={21},
  number={16},
  pages={17581-17590},
  doi={10.1109/JSEN.2021.3071290}}

@ARTICLE{9804867,
  author={Soomro, Toufique A. and Zheng, Lihong and Afifi, Ahmed J. and Ali, Ahmed and Soomro, Shafiullah and Yin, Ming and Gao, Junbin},
  journal={IEEE Reviews in Biomedical Engineering}, 
  title={Image Segmentation for MR Brain Tumor Detection Using Machine Learning: A Review}, 
  year={2023},
  volume={16},
  number={},
  pages={70-90},
  doi={10.1109/RBME.2022.3185292}}



@misc{huang2020unet,
      title={UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation}, 
      author={Huimin Huang and Lanfen Lin and Ruofeng Tong and Hongjie Hu and Qiaowei Zhang and Yutaro Iwamoto and Xianhua Han and Yen-Wei Chen and Jian Wu},
      year={2020},
      eprint={2004.08790},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{liu2021random,
      title={Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond}, 
      author={Fanghui Liu and Xiaolin Huang and Yudong Chen and Johan A. K. Suykens},
      year={2021},
      eprint={2004.11154},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@article{DBLP:journals/corr/abs-1708-07747,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  archivePrefix = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{KUTATELADZE2022165,
title = {The kernel trick for nonlinear factor modeling},
journal = {International Journal of Forecasting},
volume = {38},
number = {1},
pages = {165-177},
year = {2022},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169207021000741},
author = {Varlam Kutateladze},
keywords = {Macroeconomic forecasting, Latent factor model, Nonlinear time series, Principal component analysis, Kernel PCA, Neural networks, Econometric models},
abstract = {Factor modeling is a powerful statistical technique that permits common dynamics to be captured in a large panel of data with a few latent variables, or factors, thus alleviating the curse of dimensionality. Despite its popularity and widespread use for various applications ranging from genomics to finance, this methodology has predominantly remained linear. This study estimates factors nonlinearly through the kernel method, which allows for flexible nonlinearities while still avoiding the curse of dimensionality. We focus on factor-augmented forecasting of a single time series in a high-dimensional setting, known as diffusion index forecasting in macroeconomics literature. Our main contribution is twofold. First, we show that the proposed estimator is consistent and it nests the linear principal component analysis estimator as well as some nonlinear estimators introduced in the literature as specific examples. Second, our empirical application to a classical macroeconomic dataset demonstrates that this approach can offer substantial advantages over mainstream methods.}
}


@Article{technologies10040090,
AUTHOR = {Rizzoli, Giulia and Barbato, Francesco and Zanuttigh, Pietro},
TITLE = {Multimodal Semantic Segmentation in Autonomous Driving: A Review of Current Approaches and Future Perspectives},
JOURNAL = {Technologies},
VOLUME = {10},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {90},
URL = {https://www.mdpi.com/2227-7080/10/4/90},
ISSN = {2227-7080},
ABSTRACT = {The perception of the surrounding environment is a key requirement for autonomous driving systems, yet the computation of an accurate semantic representation of the scene starting from RGB information alone is very challenging. In particular, the lack of geometric information and the strong dependence on weather and illumination conditions introduce critical challenges for approaches tackling this task. For this reason, most autonomous cars exploit a variety of sensors, including color, depth or thermal cameras, LiDARs, and RADARs. How to efficiently combine all these sources of information to compute an accurate semantic description of the scene is still an unsolved task, leading to an active research field. In this survey, we start by presenting the most commonly employed acquisition setups and datasets. Then we review several different deep learning architectures for multimodal semantic segmentation. We will discuss the various techniques to combine color, depth, LiDAR, and other modalities of data at different stages of the learning architectures, and we will show how smart fusion strategies allow us to improve performances with respect to the exploitation of a single source of information.},
DOI = {10.3390/technologies10040090}
}


@article{mairal2018machine,
  title={Machine learning with kernel methods},
  author={Mairal, Julien and Vert, Jean-Philippe},
  journal={Lecture Notes, January},
  volume={10},
  pages={52},
  year={2018}
}


@article{ WOS:000673109800001,
Author = {Khan, Muhammad Zubair and Gajendran, Mohan Kumar and Lee, Yugyung and
   Khan, Muazzam A.},
Title = {Deep Neural Architectures for Medical Image Semantic Segmentation:
   Review},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {83002-83024},
Abstract = {Deep learning has an enormous impact on medical image analysis. Many
   computer-aided diagnostic systems equipped with deep networks are
   rapidly reducing human intervention in healthcare. Among several
   applications, medical image semantic segmentation is one of the core
   areas of active research to delineate the anatomical structures and
   other regions of interest. It has a significant contribution to
   healthcare and provides guided interventions, radiotherapy, and improved
   radiological diagnostics. The underlying article provides a brief
   overview of deep convolutional neural architecture, the platforms and
   applications of deep neural networks, metrics used for empirical
   evaluation, state-of-the-art semantic segmentation architectures based
   on a foundational convolution concept, and a review of publicly
   available medical image datasets highlighting four distinct regions of
   interest. The article also analyzes the existing work and provides
   open-ended potential research directions in deep medical image semantic
   segmentation.},
Publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
Address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
Type = {Review},
Language = {English},
Affiliation = {Khan, MZ (Corresponding Author), Univ Missouri, Sch Comp \& Engn, Kansas City, MO 64110 USA.
   Khan, MA (Corresponding Author), Quaid I Azam Univ, Dept Comp Sci, Islamabad 44000, Pakistan.
   Khan, Muhammad Zubair; Gajendran, Mohan Kumar; Lee, Yugyung, Univ Missouri, Sch Comp \& Engn, Kansas City, MO 64110 USA.
   Khan, Muazzam A., Quaid I Azam Univ, Dept Comp Sci, Islamabad 44000, Pakistan.},
DOI = {10.1109/ACCESS.2021.3086530},
ISSN = {2169-3536},
Keywords = {Image segmentation; Medical diagnostic imaging; Computer architecture;
   Semantics; Convolution; Recurrent neural networks; Deep learning; Deep
   learning; convolution neural network; medical image analysis; semantic
   segmentation; skip-connections; encoder-decoder; computer-aided
   diagnostics; healthcare},
Keywords-Plus = {COMPUTER-AIDED DIAGNOSIS; FALSE-POSITIVE REDUCTION; AUTOMATIC DETECTION;
   PULMONARY NODULES; CT IMAGES; U-NET; MITOCHONDRIA SEGMENTATION;
   CONVOLUTIONAL NETWORKS; EM IMAGES; MODEL},
Research-Areas = {Computer Science; Engineering; Telecommunications},
Web-of-Science-Categories  = {Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications},
Author-Email = {mkzb3@mail.umkc.edu
   muazzam.khattak@qau.edu.pk},
Affiliations = {University of Missouri System; University of Missouri Kansas City; Quaid
   I Azam University},
ResearcherID-Numbers = {Khattak, Muazzam A. Khan/AAF-1494-2020
   },
ORCID-Numbers = {Khattak, Muazzam A. Khan/0000-0001-6140-1201
   Khan, Muhammad/0000-0002-2625-4936
   Gajendran, Mohan Kumar/0000-0003-4757-7723},
Funding-Acknowledgement = {School of Graduate Studies, UMKC, USA; Higher Education Commission,
   Pakistan},
Funding-Text = {This work was supported in part by the School of Graduate Studies, UMKC,
   USA, and in part by the Higher Education Commission, Pakistan, under
   US-Pakistan Knowledge Corridor Research Grant.},
Number-of-Cited-References = {212},
Times-Cited = {17},
Usage-Count-Last-180-days = {25},
Usage-Count-Since-2013 = {139},
Journal-ISO = {IEEE Access},
Doc-Delivery-Number = {TI9IW},
Web-of-Science-Index = {Science Citation Index Expanded (SCI-EXPANDED)},
Unique-ID = {WOS:000673109800001},
OA = {gold},
DA = {2023-04-13},
}


@article{Ibtehaz_2020,
   title={MultiResUNet: Rethinking the U-Net architecture for multimodal biomedical image segmentation},
   volume={121},
   ISSN={0893-6080},
   url={http://dx.doi.org/10.1016/j.neunet.2019.08.025},
   DOI={10.1016/j.neunet.2019.08.025},
   journal={Neural Networks},
   publisher={Elsevier BV},
   author={Ibtehaz, Nabil and Rahman, M. Sohel},
   year={2020},
   month={Jan},
   pages={74–87}
}

@misc{nakkiran2019deep,
      title={Deep Double Descent: Where Bigger Models and More Data Hurt}, 
      author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
      year={2019},
      eprint={1912.02292},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{sutherland2015error,
      title={On the Error of Random Fourier Features}, 
      year={2015},
      eprint={1506.02785},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{YEUNG2022102026,
title = {Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation},
journal = {Computerized Medical Imaging and Graphics},
volume = {95},
pages = {102026},
year = {2022},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2021.102026},
url = {https://www.sciencedirect.com/science/article/pii/S0895611121001750},
author = {Michael Yeung and Evis Sala and Carola-Bibiane Schönlieb and Leonardo Rundo},
keywords = {Loss function, Class imbalance, Machine learning, Convolutional neural networks, Medical image segmentation},
abstract = {Automatic segmentation methods are an important advancement in medical image analysis. Machine learning techniques, and deep neural networks in particular, are the state-of-the-art for most medical image segmentation tasks. Issues with class imbalance pose a significant challenge in medical datasets, with lesions often occupying a considerably smaller volume relative to the background. Loss functions used in the training of deep learning algorithms differ in their robustness to class imbalance, with direct consequences for model convergence. The most commonly used loss functions for segmentation are based on either the cross entropy loss, Dice loss or a combination of the two. We propose the Unified Focal loss, a new hierarchical framework that generalises Dice and cross entropy-based losses for handling class imbalance. We evaluate our proposed loss function on five publicly available, class imbalanced medical imaging datasets: CVC-ClinicDB, Digital Retinal Images for Vessel Extraction (DRIVE), Breast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and Kidney Tumour Segmentation 2019 (KiTS19). We compare our loss function performance against six Dice or cross entropy-based loss functions, across 2D binary, 3D binary and 3D multiclass segmentation tasks, demonstrating that our proposed loss function is robust to class imbalance and consistently outperforms the other loss functions. Source code is available at: https://github.com/mlyg/unified-focal-loss.}
}

@article{BOUVET202053,
title = {Infrared thermography to assess dermatomal levels of labor epidural analgesia with 1 mg/mL ropivacaine plus 0.5 µg/mL sufentanil: a prospective cohort study},
journal = {International Journal of Obstetric Anesthesia},
volume = {41},
pages = {53-58},
year = {2020},
issn = {0959-289X},
doi = {https://doi.org/10.1016/j.ijoa.2019.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959289X19305291},
author = {L. Bouvet and M. Roukhomovsky and F.-P. Desgranges and B. Allaouchiche and D. Chassard},
keywords = {Thermography, Epidural analgesia, Obstetric analgesia},
abstract = {Background
Assessment of the effectiveness of obstetric epidural analgesia may be difficult and techniques for objective assessment of epidural spread of local anesthetic would be useful. In this prospective cohort study we assessed whether obstetric epidural analgesia from a low concentration of ropivacaine led to significant change in cutaneous temperature, related to sympathetic block detected by infrared thermography, at dermatomes C4, T4, T10, L2 and L5.
Methods
Women in spontaneous labor who requested epidural analgesia were consecutively recruited. Epidural analgesia was induced with a bolus of 10–15 mL of ropivacaine 1 mg/mL and sufentanil 0.5 µg/mL, followed by continuous epidural infusion. Skin temperature was measured using thermography before and 20 min after the epidural bolus. The verbal pain score using a numeric rating scale was recorded before and 60 min after the epidural bolus. The upper sensory block to cold sensation was tested 30 and 60 min after the bolus by a physician blinded to the skin temperature. Failed epidural analgesia was defined as verbal pain score >3 at 60 min.
Results
Fifty-three parturients were included and analyzed. We found a significant increase in skin temperatures measured at T4, T10, L2 and L5 dermatomes, but not at C4, and a significant difference in the change in skin temperature at T10 between failed (n=3) and successful (n=50) epidural analgesia.
Conclusions
These results suggest that infrared thermography might be useful for the early diagnosis of successful obstetric epidural analgesia.}
}


@inproceedings{Chen2020UNetAD,

  title={$\alpha$-UNet++: A Data-Driven Neural Network Architecture for Medical Image Segmentation},
  
  author={Yaxin Chen and Benteng Ma and Yong Xia},
  
  booktitle={DART/DCL@MICCAI},
  
  year={2020}
}

@article{Zunair2021,
   abstract = {The U-Net architecture, built upon the fully convolutional network, has proven to be effective in biomedical image segmentation. However, U-Net applies skip connections to merge semantically different low- and high-level convolutional features, resulting in not only blurred feature maps, but also over- and under-segmented target regions. To address these limitations, we propose a simple, yet effective end-to-end depthwise encoder-decoder fully convolutional network architecture, called Sharp U-Net, for binary and multi-class biomedical image segmentation. The key rationale of Sharp U-Net is that instead of applying a plain skip connection, a depthwise convolution of the encoder feature map with a sharpening kernel filter is employed prior to merging the encoder and decoder features, thereby producing a sharpened intermediate feature map of the same size as the encoder map. Using this sharpening filter layer, we are able to not only fuse semantically less dissimilar features, but also to smooth out artifacts throughout the network layers during the early stages of training. Our extensive experiments on six datasets show that the proposed Sharp U-Net model consistently outperforms or matches the recent state-of-the-art baselines in both binary and multi-class segmentation tasks, while adding no extra learnable parameters. Furthermore, Sharp U-Net outperforms baselines that have more than three times the number of learnable parameters.},
   author = {Hasib Zunair and A. Ben Hamza},
   doi = {10.1016/J.COMPBIOMED.2021.104699},
   issn = {0010-4825},
   journal = {Computers in Biology and Medicine},
   keywords = {Fully convolutional network,Semantic segmentation,Sharpening filter,Skip connections,U-Net},
   month = {9},
   pages = {104699},
   pmid = {34348214},
   publisher = {Pergamon},
   title = {Sharp U-Net: Depthwise convolutional network for biomedical image segmentation},
   volume = {136},
   year = {2021},
}




@misc{jafari2020drunet,
      title={DRU-net: An Efficient Deep Convolutional Neural Network for Medical Image Segmentation}, 
      author={Mina Jafari and Dorothee Auer and Susan Francis and Jonathan Garibaldi and Xin Chen},
      year={2020},
      eprint={2004.13453},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}




@misc{sun2020saunet,
      title={SAUNet: Shape Attentive U-Net for Interpretable Medical Image Segmentation}, 
      author={Jesse Sun and Fatemeh Darbehani and Mark Zaidi and Bo Wang},
      year={2020},
      eprint={2001.07645},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}


@misc{williams2022automatic,
      title={Automatic quality control framework for more reliable integration of machine learning-based image segmentation into medical workflows}, 
      author={Elena Williams and Sebastian Niehaus and Janis Reinelt and Alberto Merola and Paul Glad Mihai and Kersten Villringer and Konstantin Thierbach and Evelyn Medawar and Daniel Lichterfeld and Ingo Roeder and Nico Scherf and Maria del C. Valdés Hernández},
      year={2022},
      eprint={2112.03277},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}


@article{Wang_2022,
	doi = {10.1049/ipr2.12419},
	url = {https://doi.org/10.1049\%2Fipr2.12419},
	year = 2022,
	month = {jan},
	publisher = {Institution of Engineering and Technology ({IET})},
	volume = {16},
	number = {5},
	pages = {1243--1267},
	author = {Risheng Wang and Tao Lei and Ruixia Cui and Bingtao Zhang and Hongying Meng and Asoke K. Nandi},
	title = {Medical image segmentation using deep learning: A survey},
	journal = {{IET} Image Processing}
}



@misc{sacha2023protoseg,
      title={ProtoSeg: Interpretable Semantic Segmentation with Prototypical Parts}, 
      author={Mikołaj Sacha and Dawid Rymarczyk and Łukasz Struski and Jacek Tabor and Bartosz Zieliński},
      year={2023},
      eprint={2301.12276},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



@article{Schorr2021,
   abstract = {Trust in artificial intelligence (AI) predictions is a crucial point for a widespread acceptance of new technologies, especially in sensitive areas like autonomous driving. The need for tools explaining AI for deep learning of images is thus eminent. Our proposed toolbox Neuroscope addresses this demand by offering state-of-the-art visualization algorithms for image classification and newly adapted methods for semantic segmentation of convolutional neural nets (CNNs). With its easy to use graphical user interface (GUI), it provides visualization on all layers of a CNN. Due to its open model-view-controller architecture, networks generated and trained with Keras and PyTorch are processable, with an interface allowing extension to additional frameworks. We demonstrate the explanation abilities provided by Neuroscope using the example of traffic scene analysis.},
   author = {Christian Schorr and Payman Goodarzi and Fei Chen and Tim Dahmen},
   doi = {10.3390/APP11052199},
   issn = {2076-3417},
   issue = {5},
   journal = {Applied Sciences 2021, Vol. 11, Page 2199},
   keywords = {convolutional neural nets,explainable AI,image classification,semantic segmentation},
   month = {3},
   pages = {2199},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Neuroscope: An Explainable AI Toolbox for Semantic Segmentation and Image Classification of Convolutional Neural Nets},
   volume = {11},
   url = {https://www.mdpi.com/2076-3417/11/5/2199/htm https://www.mdpi.com/2076-3417/11/5/2199},
   year = {2021},
}


@inproceedings{dardouillet2022explainability,
  title={Explainability of Image Semantic Segmentation Through SHAP Values},
  author={Dardouillet, Pierre and Benoit, Alexandre and Amri, Emna and Bolon, Philippe and Dubucq, Dominique and Cr{\'e}doz, Anthony},
  booktitle={ICPR-XAIE},
  year={2022}
}


@article{Ventura2023,
   abstract = {The accuracy and flexibility of Deep Convolutional Neural Networks (DCNNs) have been highly validated over the past years. However, their intrinsic opaqueness is still affecting their reliability and limiting their application in critical production systems, where the black-box behavior is difficult to be accepted. This work proposes EBAnO, an innovative explanation framework able to analyze the decision-making process of DCNNs in image classification by providing prediction-local and class-based model-wise explanations through the unsupervised mining of knowledge contained in multiple convolutional layers. EBAnO provides detailed visual and numerical explanations thanks to two specific indexes that measure the features’ influence and their influence precision in the decision-making process. The framework has been experimentally evaluated, both quantitatively and qualitatively, by (i) analyzing its explanations with four state-of-the-art DCNN architectures, (ii) comparing its results with three state-of-the-art explanation strategies and (iii) assessing its effectiveness and easiness of understanding through human judgment, by means of an online survey. EBAnO has been released as open-source code and it is freely available online.},
   author = {Francesco Ventura and Salvatore Greco and Daniele Apiletti and Tania Cerquitelli},
   doi = {10.1007/S10618-023-00915-X/FIGURES/26},
   issn = {1573756X},
   journal = {Data Mining and Knowledge Discovery},
   keywords = {Black-box classifier,Deep convolutional neural network,Explainability,Explainable artificial intelligence,Image classification},
   month = {2},
   pages = {1-58},
   publisher = {Springer},
   title = {Explaining deep convolutional models by measuring the influence of interpretable features in image classification},
   url = {https://link.springer.com/article/10.1007/s10618-023-00915-x},
   year = {2023},
}


@misc{wang2023semanticguided,
      title={Semantic-guided Disentangled Representation for Unsupervised Cross-modality Medical Image Segmentation}, 
      author={Shuai Wang and Rui Li},
      year={2023},
      eprint={2203.14025},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@InProceedings{Kim_2020_CVPR,
author = {Kim, Myeongjin and Byun, Hyeran},
title = {Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


@inproceedings{he2021cap,
  title={Cap: Context-aware pruning for semantic segmentation},
  author={He, Wei and Wu, Meiqing and Liang, Mingfu and Lam, Siew-Kei},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={960--969},
  year={2021}
}


@InProceedings{Lv_2020_CVPR,
author = {Lv, Fengmao and Liang, Tao and Chen, Xiang and Lin, Guosheng},
title = {Cross-Domain Semantic Segmentation via Domain-Invariant Interactive Relation Transfer},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{ghosh2019scale,
      title={Scale Steerable Filters for Locally Scale-Invariant Convolutional Neural Networks}, 
      author={Rohan Ghosh and Anupam K. Gupta},
      year={2019},
      eprint={1906.03861},
      archivePrefix={arXiv},
      primaryClass={cs.CV}

}
@article{Galati2022FromAT,
  title={From Accuracy to Reliability and Robustness in Cardiac Magnetic Resonance Image Segmentation: A Review},
  author={Francesco Galati and S{\'e}bastien Ourselin and Maria A. Zuluaga},
  journal={Applied Sciences},
  year={2022}
}

@article{Jain2020OverviewAI,
  title={Overview and Importance of Data Quality for Machine Learning Tasks},
  author={Abhinav Jain and Hima Patel and Lokesh Nagalapatti and Nitin Gupta and Sameep Mehta and Shanmukha C. Guttula and Shashank Mujumdar and Shazia Afzal and Ruhi Sharma Mittal and Vitobha Munigala},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020}
}

@misc{chang2020mixupcam,
      title={Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization}, 
      author={Yu-Ting Chang and Qiaosong Wang and Wei-Chih Hung and Robinson Piramuthu and Yi-Hsuan Tsai and Ming-Hsuan Yang},
      year={2020},
      eprint={2008.01201},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{ZHANG202041,
title = {Understanding the learning mechanism of convolutional neural networks in spectral analysis},
journal = {Analytica Chimica Acta},
volume = {1119},
pages = {41-51},
year = {2020},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2020.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S0003267020303767},
author = {Xiaolei Zhang and Jinfan Xu and Jie Yang and Li Chen and Haibo Zhou and Xiangjiang Liu and Haifeng Li and Tao Lin and Yibin Ying},
keywords = {Feature visualization, Interpretation, Deep learning, Reliability, Class activation mapping},
abstract = {Deep learning approaches, especially convolutional neural network (CNN) models, have achieved excellent performances in vibrational spectral analysis. The critical drawback of the CNN approach is the lack of interpretation, and it is regarded as a black box. Interpreting the learning mechanism of chemometric models is critical for intuitive understanding and further application. In this study, an interpretable CNN model with a global average pooling layer is presented for Raman and mid-infrared spectral data analysis. A class activation mapping (CAM)-based approach is leveraged to visualize the active variables in the whole spectrum. The visualization of active variables shows a discriminative pattern in which the most contributed variables peaked around theoretical chemical characteristic bands. The visualization of the feature maps by three convolutional layers demonstrates the data transformation pipeline and how the CNN model hierarchically extracts informative spectral features. The first layer acts as a Savitzky-Golay filter and learns spectral shape characteristics, while the second layer learns enhanced patterns from typical spectral peaks on a few correlated variables. The third layer shows stable activations on critical spectral peaks. A partial least squares - linear discriminant analysis (PLS-LDA) model is presented for comparison on classification accuracy and model interpretation. The CNN model yields mean classification accuracies of 99.01 and 100\% for E. coli and meat datasets on the test set, while the PLS-LDA models obtain accuracies of 98.83 and 100\%. Both the CNN and PLS-LDA models demonstrate stable patterns on active variables while CNN models are more stable than PLS-LDA models on classification performances for various dataset partitions with Monte-Carlo cross-validation.}
}



@INPROCEEDINGS{9506582,
  author={Lin, Dongyun and Li, Yiqun and Prasad, Shitala and Nwe, Tin Lay and Dong, Sheng and Oo, Zaw Min},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, 
  title={Cam-Guided U-Net With Adversarial Regularization For Defect Segmentation}, 
  year={2021},
  volume={},
  number={},
  pages={1054-1058},
  doi={10.1109/ICIP42928.2021.9506582}}

@article{MILLER2022100598,
title = {Bayesian spatial modeling using random Fourier frequencies},
journal = {Spatial Statistics},
volume = {48},
pages = {100598},
year = {2022},
issn = {2211-6753},
doi = {https://doi.org/10.1016/j.spasta.2022.100598},
url = {https://www.sciencedirect.com/science/article/pii/S2211675322000069},
author = {Matthew J. Miller and Brian J. Reich},
keywords = {Bayesian statistics, Low-rank approximations, Spectral methods, Hamiltonian Monte Carlo},
abstract = {Spectral methods are important for both theory and computation in spatial data analysis. When data lie on a grid, spectral approaches can take advantage of the discrete Fourier transform for fast computation. If data are not on a grid, then low-rank processes with Fourier basis functions may be sufficient approximations. However, deciding which basis functions to use is difficult and can depend on unknown parameters. Here, we introduce Bayesian Random Fourier Frequencies (BRFF), a fully Bayesian extension of the random Fourier features approach. BRFF treats the spectral frequencies as random parameters, which unlike fixed frequency approximations allows the frequencies to be data-adaptive and averages over uncertainty in frequency selection. We apply this method to non-gridded continuous, binary, and count data. We compare BRFF using simulated and observed data to another popular low-rank method, the predictive processes (PP) model. BRFF is faster than PP, and outperforms or matches the predictive performance of the PP model in settings with high numbers of observations.}
}

@misc{Gundersen, title={Random Fourier Features},
year={2019},
url={https://gregorygundersen.com/blog/2019/12/23/random-fourier-features/}, journal={Random fourier features}, author={Gundersen, Gregory}} 




@misc{dushatskiy2022data,
      title={Data variation-aware medical image segmentation}, 
      author={Arkadiy Dushatskiy and Gerry Lowe and Peter A. N. Bosman and Tanja Alderliesten},
      year={2022},
      eprint={2202.12099},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

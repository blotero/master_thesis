%FCN  

@article{Zhou2021,
  author   = {Zhou, S. Kevin and Greenspan, Hayit and Davatzikos, Christos and Duncan, James S. and Van Ginneken, Bram and Madabhushi, Anant and Prince, Jerry L. and Rueckert, Daniel and Summers, Ronald M.},
  journal  = {Proceedings of the IEEE},
  title    = {A Review of Deep Learning in Medical Imaging: Imaging Traits, Technology Trends, Case Studies With Progress Highlights, and Future Promises},
  year     = {2021},
  volume   = {109},
  number   = {5},
  pages    = {820-838},
  keywords = {Biomedical imaging;Medical diagnostic imaging;Image segmentation;Diseases;Deep learning;Medical services;Computed tomography;Artificial intelligence;Market research;Network architecture;Clinical diagnosis;Deep learning (DL);medical imaging;survey},
  doi      = {10.1109/JPROC.2021.3054390}
}


@article{Panayides2020,
  author   = {Panayides, Andreas S. and Amini, Amir and Filipovic, Nenad D. and Sharma, Ashish and Tsaftaris, Sotirios A. and Young, Alistair and Foran, David and Do, Nhan and Golemati, Spyretta and Kurc, Tahsin and Huang, Kun and Nikita, Konstantina S. and Veasey, Ben P. and Zervakis, Michalis and Saltz, Joel H. and Pattichis, Constantinos S.},
  journal  = {IEEE Journal of Biomedical and Health Informatics},
  title    = {AI in Medical Imaging Informatics: Current Challenges and Future Directions},
  year     = {2020},
  volume   = {24},
  number   = {7},
  pages    = {1837-1857},
  keywords = {Biomedical imaging;X-ray imaging;Magnetic resonance imaging;Computed tomography;Informatics;Three-dimensional displays;Medical Imaging;Image Analysis;Image Classification;Image Processing;Image Segmentation;Image Visualization;Integrative Analytics;Machine Learning;Deep Learning;Big Data},
  doi      = {10.1109/JBHI.2020.2991043}
}
@article{Azad2024,
  author   = {Azad, Reza and Aghdam, Ehsan Khodapanah and Rauland, Amelie and Jia, Yiwei and Avval, Atlas Haddadi and Bozorgpour, Afshin and Karimijafarbigloo, Sanaz and Cohen, Joseph Paul and Adeli, Ehsan and Merhof, Dorit},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Medical Image Segmentation Review: The Success of U-Net},
  year     = {2024},
  volume   = {46},
  number   = {12},
  pages    = {10076-10095},
  keywords = {Image segmentation;Biomedical imaging;Taxonomy;Computer architecture;Feature extraction;Transformers;Task analysis;Convolutional neural network;deep learning;medical image segmentation;transformer;U-Net},
  doi      = {10.1109/TPAMI.2024.3435571}
}

@article{XuYan2024,
  author         = {Xu, Yan and Quan, Rixiang and Xu, Weiting and Huang, Yi and Chen, Xiaolong and Liu, Fengyuan},
  title          = {Advances in Medical Image Segmentation: A Comprehensive Review of Traditional, Deep Learning and Hybrid Approaches},
  journal        = {Bioengineering},
  volume         = {11},
  year           = {2024},
  number         = {10},
  article-number = {1034},
  url            = {https://www.mdpi.com/2306-5354/11/10/1034},
  pubmedid       = {39451409},
  issn           = {2306-5354},
  abstract       = {Medical image segmentation plays a critical role in accurate diagnosis and treatment planning, enabling precise analysis across a wide range of clinical tasks. This review begins by offering a comprehensive overview of traditional segmentation techniques, including thresholding, edge-based methods, region-based approaches, clustering, and graph-based segmentation. While these methods are computationally efficient and interpretable, they often face significant challenges when applied to complex, noisy, or variable medical images. The central focus of this review is the transformative impact of deep learning on medical image segmentation. We delve into prominent deep learning architectures such as Convolutional Neural Networks (CNNs), Fully Convolutional Networks (FCNs), U-Net, Recurrent Neural Networks (RNNs), Adversarial Networks (GANs), and Autoencoders (AEs). Each architecture is analyzed in terms of its structural foundation and specific application to medical image segmentation, illustrating how these models have enhanced segmentation accuracy across various clinical contexts. Finally, the review examines the integration of deep learning with traditional segmentation methods, addressing the limitations of both approaches. These hybrid strategies offer improved segmentation performance, particularly in challenging scenarios involving weak edges, noise, or inconsistent intensities. By synthesizing recent advancements, this review provides a detailed resource for researchers and practitioners, offering valuable insights into the current landscape and future directions of medical image segmentation.},
  doi            = {10.3390/bioengineering11101034}
}

@article{Bhalgat2018,
  author     = {Yash Bhalgat and
                Meet P. Shah and
                Suyash P. Awate},
  title      = {Annotation-cost Minimization for Medical Image Segmentation using
                Suggestive Mixed Supervision Fully Convolutional Networks},
  journal    = {CoRR},
  volume     = {abs/1812.11302},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.11302},
  eprinttype = {arXiv},
  eprint     = {1812.11302},
  timestamp  = {Thu, 07 Mar 2024 15:32:46 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-11302.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Shah2018,
  author    = {Shah, Meet P.
               and Merchant, S. N.
               and Awate, Suyash P.},
  editor    = {Frangi, Alejandro F.
               and Schnabel, Julia A.
               and Davatzikos, Christos
               and Alberola-L{\'o}pez, Carlos
               and Fichtinger, Gabor},
  title     = {MS-Net: Mixed-Supervision Fully-Convolutional Networks for Full-Resolution Segmentation},
  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},
  year      = {2018},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {379--387},
  abstract  = {For image segmentation, typical fully convolutional networks (FCNs) need strong supervision through a large sample of high-quality dense segmentations, entailing high costs in expert-raters' time and effort. We propose MS-Net, a new FCN to significantly reduce supervision cost, and improve performance, by coupling strong supervision with weak supervision through low-cost input in the form of bounding boxes and landmarks. Our MS-Net enables instance-level segmentation at high spatial resolution, with feature extraction using dilated convolutions. We propose a new loss function using bootstrapped Dice overlap for precise segmentation. Results on large datasets show that MS-Net segments more accurately at reduced supervision costs, compared to the state of the art.},
  isbn      = {978-3-030-00937-3}
}
@article{LuEtAl2023,
  title        = {Rethinking Quality Assurance for Crowdsourced Multi-ROI Image Segmentation},
  volume       = {11},
  url          = {https://ojs.aaai.org/index.php/HCOMP/article/view/27552},
  doi          = {10.1609/hcomp.v11i1.27552},
  abstractnote = {Collecting high quality annotations to construct an evaluation dataset is essential for assessing the true performance of machine learning models. One popular way of performing data annotation is via crowdsourcing, where quality can be of concern. Despite much prior work addressing the annotation quality problem in crowdsourcing generally, little has been discussed in detail for image segmentation tasks. These tasks often require pixel-level annotation accuracy, and is relatively complex when compared to image classification or object detection with bounding-boxes. In this paper, we focus on image segmentation annotation via crowdsourcing, where images may not have been collected in a controlled way. In this setting, the task of annotating may be non-trivial, where annotators may experience difficultly in differentiating between regions-of-interest (ROIs) and background pixels. We implement an annotation process and examine the effectiveness of several in-situ and manual quality assurance and quality control mechanisms. We implement an annotation process on a medical image annotation task and examine the effectiveness of several in-situ and manual quality assurance and quality control mechanisms. Our observations on this task are three-fold. Firstly, including an onboarding and a pilot phase improves quality assurance as annotators can familiarize themselves with the task, especially when the definition of ROIs is ambiguous. Secondly, we observe high variability of annotation times, leading us to believe it cannot be relied upon as a source of information for quality control. When performing agreement analysis, we also show that global-level inter-rater agreement is insufficient to provide useful information, especially when annotator skill levels vary. Thirdly, we recognize that reviewing all annotations can be time-consuming and often infeasible, and there currently exist no mechanisms to reduce the workload for reviewers. Therefore, we propose a method to create a priority list of images for review based on inter-rater agreement. Our experiments suggest that this method can be used to improve reviewer efficiency when compared to a baseline approach, especially if a fixed work budget is required.},
  number       = {1},
  journal      = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  author       = {Lu, Xiaolu and Ratcliffe, David and Kao, Tsu-Ting and Tikhonov, Aristarkh and Litchfield, Lester and Rodger, Craig and Wang, Kaier},
  year         = {2023},
  month        = {Nov.},
  pages        = {103-114}
}

@inproceedings{Lopez2023,
  author    = {L{\'o}pez-P{\'e}rez, Miguel
               and Morales-{\'A}lvarez, Pablo
               and Cooper, Lee A. D.
               and Molina, Rafael
               and Katsaggelos, Aggelos K.},
  editor    = {Juarez, Jose M.
               and Marcos, Mar
               and Stiglic, Gregor
               and Tucker, Allan},
  title     = {Crowdsourcing Segmentation of Histopathological Images Using Annotations Provided by Medical Students},
  booktitle = {Artificial Intelligence in Medicine},
  year      = {2023},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {245--249},
  abstract  = {Segmentation of histopathological images is an essential task for cancer diagnosis and prognosis in computational pathology. Unfortunately, Machine Learning (ML) techniques need large labeled datasets to train accurate segmentation algorithms that generalize well. A possible solution to alleviate this burden is crowdsourcing, which distributes the effort of labeling among a group of (non-expert) individuals. The bias and noise from less experienced annotators may hamper the performance of machine learning techniques. So far, crowdsourcing approaches in ML leveraging these noisy labels achieve promising results in classification. However, crowdsourcing segmentation is still a challenge in histopathological images. This paper presents a novel crowdsourcing approach to the segmentation of Triple Negative Breast Cancer images. Our method is based on the UNet architecture incorporating a pre-trained ResNet-34 as a backbone. The noisy behavior of the annotators is modeled with a coupled network. Our methodology is validated on a real-world dataset annotated by medical students, where five classes are distinguished. The results show that our method with crowd labels achieves a high level of accuracy in segmentation (DICE: 0.7578), outperforming the well-known STAPLE (DICE: 0.7039) and close to the segmentation model using expert labels (DICE: 0.7723). In conclusion, the initial results of our work suggest that crowdsourcing is a feasible approach to segmentation in histopathological images https://github.com/wizmik12/CRowd{\_}Seg.},
  isbn      = {978-3-031-34344-5}
}
% Used for producing comparative-tasks-and-medical-image-types

@article{YuEtAl2025,
  author   = {Yu, Jiejiang and Li, Bingbing and Pan, Xipeng and Shi, Zhenwei and Wang, Huadeng and Lan, Rushi and Luo, Xiaonan},
  journal  = {IEEE Journal of Biomedical and Health Informatics},
  title    = {Semi-supervised Gland Segmentation via Feature-enhanced Contrastive Learning and Dual-consistency Strategy},
  year     = {2025},
  volume   = {},
  number   = {},
  pages    = {1-11},
  keywords = {Glands;Image segmentation;Predictive models;Contrastive learning;Feature extraction;Data models;Biomedical imaging;Training;Semisupervised learning;Artificial intelligence;semi-supervised learning;gland segmentation;contrastive learning;pseudo label reweighting},
  doi      = {10.1109/JBHI.2025.3546698}
}


@article{Brito-Pacheco2025,
  author       = {Brito-Pacheco, Daniel and Giannopoulos, Panos and Reyes-Aldasoro, Constantino Carlos},
  title        = {Persistent Homology in Medical Image Processing: A Literature Review},
  elocation-id = {2025.02.21.25322669},
  year         = {2025},
  doi          = {10.1101/2025.02.21.25322669},
  publisher    = {Cold Spring Harbor Laboratory Press},
  abstract     = {Medical image analysis has experienced significant advances with the integration of machine learning, deep learning, and other mathematical and computational methodologies into the pipelines of data analysis. One methodology that has received less attention is Persistent Homology (PH), which comes from the growing field of Topological Data Analysis and has the ability to extract features from data at different scales and build multi-scale summaries. In this work, we present a systematic review of PH applied in medical images. To illustrate the potential of PH, we introduce the main concepts of PH and demonstrate with an example of histopathology. Fifteen articles where PH was applied to medical image analysis tasks such as segmentation and classification were selected and reviewed. It was observed that PH is very versatile, as it can be applied in many different contexts and to different data types, whilst also showing great potential in increasing model accuracy in both classification and segmentation. It was also observed that image segmentation predominantly uses basic level-set filtration to calculate PH, while classification takes various approaches using filtration on more complex structures built from data. This review highlights PH as an important tool that can further advance medical image analysis.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis study did not receive any funding.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:https://zenodo.org/records/1214456I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesAll data produced in the present work are available upon request to the authors. https://zenodo.org/records/1214456}
}
@article{RyouEtAl2025,
  author   = {Ryou, Hosuk and Thomas, Emily and Wojciechowska, Marta and Harding, Laura and Tam, Ka Ho and Wang, Ruoyu and Hu, Xuezi and Rittscher, Jens and Cooper, Rosalin and Royston, Daniel},
  title    = {Reticulin-Free Quantitation of Bone Marrow Fibrosis in MPNs: Utility and Applications},
  journal  = {eJHaem},
  volume   = {6},
  number   = {2},
  pages    = {e70005},
  keywords = {bone marrow pathology, diagnostic haematology, haematological malignancy, marrow fibrosis, myeloproliferative disease, machine learning},
  doi      = {https://doi.org/10.1002/jha2.70005},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jha2.70005},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jha2.70005},
  abstract = {ABSTRACT Background Automated quantitation of marrow fibrosis promises to improve fibrosis assessment in myeloproliferative neoplasms (MPNs). However, analysis of reticulin-stained images is complicated by technical challenges within laboratories and variability between institutions. Methods We have developed a machine learning model that can quantitatively assess fibrosis directly from H\&E-stained bone marrow trephine tissue sections. Results Our haematoxylin and eosin (H\&E)-based fibrosis quantitation model demonstrates comparable performance to an existing reticulin-stained model (Continuous Indexing of Fibrosis [CIF]) while benefitting from the improved tissue retention and staining characteristics of H\&E-stained sections. Conclusions H\&E-derived quantitative marrow fibrosis has potential to augment routine practice and clinical trials while supporting the emerging field of spatial multi-omic analysis.},
  year     = {2025}
}


@article{DingyiEtAl2025,
  title    = {Pathology report generation from whole slide images with knowledge retrieval and multi-level regional feature selection},
  journal  = {Computer Methods and Programs in Biomedicine},
  volume   = {263},
  pages    = {108677},
  year     = {2025},
  issn     = {0169-2607},
  doi      = {https://doi.org/10.1016/j.cmpb.2025.108677},
  url      = {https://www.sciencedirect.com/science/article/pii/S016926072500094X},
  author   = {Dingyi Hu and Zhiguo Jiang and Jun Shi and Fengying Xie and Kun Wu and Kunming Tang and Ming Cao and Jianguo Huai and Yushan Zheng},
  keywords = {Whole slide images, Pathology reports generation, Knowledge retrieval, Cross-modal representation learning},
  abstract = {Background and objectives:
              With the development of deep learning techniques, the computer-assisted pathology diagnosis plays a crucial role in clinical diagnosis. An important task within this field is report generation, which provides doctors with text descriptions of whole slide images (WSIs). Report generation from WSIs presents significant challenges due to the structural complexity and pathological diversity of tissues, as well as the large size and high information density of WSIs. The objective of this study is to design a histopathology report generation method that can efficiently generate reports from WSIs and is suitable for clinical practice.
              Methods:
              In this paper, we propose a novel approach for generating pathology reports from WSIs, leveraging knowledge retrieval and multi-level regional feature selection. To deal with the uneven distribution of pathological information in WSIs, we introduce a multi-level regional feature encoding network and a feature selection module that extracts multi-level region representations and filters out region features irrelevant to the diagnosis, enabling more efficient report generation. Moreover, we design a knowledge retrieval module to improve the report generation performance that can leverage the diagnostic information from historical cases. Additionally, we propose an out-of-domain application mode based on large language model (LLM). The use of LLM enhances the scalability of the generation model and improves its adaptability to data from different sources.
              Results:
              The proposed method is evaluated on a public datasets and one in-house dataset. On the public GastricADC (991 WSIs), our method outperforms state-of-the-art text generation methods and achieved 0.568 and 0.345 on metric Rouge-L and Bleu-4, respectively. On the in-house Gastric-3300 (3309 WSIs), our method achieved significantly better performance with Rouge-L of 0.690, which surpassed the second-best state-of-the-art method Wcap 6.3%.
              Conclusions:
              We present an advanced method for pathology report generation from WSIs, addressing the key challenges associated with the large size and complex pathological structures of these images. In particular, the multi-level regional feature selection module effectively captures diagnostically significant regions of varying sizes. The knowledge retrieval-based decoder leverages historical diagnostic data to enhance report accuracy. Our method not only improves the informativeness and relevance of the generated pathology reports but also outperforms the state-of-the-art techniques.}
}

@misc{BehnazEtAl2025,
  title         = {From Traditional to Deep Learning Approaches in Whole Slide Image Registration: A Methodological Review},
  author        = {Behnaz Elhaminia and Abdullah Alsalemi and Esha Nasir and Mostafa Jahanifar and Ruqayya Awan and Lawrence S. Young and Nasir M. Rajpoot and Fayyaz Minhas and Shan E Ahmed Raza},
  year          = {2025},
  eprint        = {2502.19123},
  archiveprefix = {arXiv},
  primaryclass  = {eess.IV},
  url           = {https://arxiv.org/abs/2502.19123}
}
@article{Avanzo2024,
  author    = {Michele Avanzo and Joseph Stancanello and Giovanni Pirrone and Annalisa Drigo and Alessandra Retico},
  title     = {The Evolution of Artificial Intelligence in Medical Imaging: From Computer Science to Machine and Deep Learning},
  journal   = {Cancers (Basel)},
  volume    = {16},
  number    = {21},
  pages     = {3702},
  year      = {2024},
  month     = {November},
  doi       = {10.3390/cancers16213702},
  pmid      = {39518140},
  pmc       = {PMC11545079},
  issn      = {2072-6694},
  publisher = {MDPI},
  address   = {Switzerland},
  url       = {https://doi.org/10.3390/cancers16213702},
  abstract  = {Artificial intelligence (AI) has evolved from early models in the 1940s to machine learning techniques like neural networks in the 1950s-60s. Recent advances, such as convolutional neural networks, increased computational power, and large datasets, have fueled AI's role in medical imaging. Initially used in expert systems, AI now aids in detecting and classifying malignant lesions. Modern techniques, including NLP, transformers, and generative models, extend AI's applications to electronic health records and image self-labeling. Despite its rapid adoption, challenges remain, including the need for clinical validation, interpretability, and ethical concerns. AI holds great promise, particularly in oncology.},
  keywords  = {artificial intelligence, deep learning, machine learning, medical imaging, neural networks},
  note      = {Author Joseph Stancanello is employed by Elekta SA. The remaining authors declare no commercial or financial conflicts of interest.},
  orcid     = {0000-0003-1711-4242, 0000-0002-6113-6644, 0000-0001-5135-4472}
}

@article{Shalf2020,
  author   = {Shalf, John },
  title    = {The future of computing beyond Moore's Law},
  journal  = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume   = {378},
  number   = {2166},
  pages    = {20190061},
  year     = {2020},
  doi      = {10.1098/rsta.2019.0061},
  url      = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2019.0061},
  eprint   = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2019.0061},
  abstract = { Moore's Law is a techno-economic model that has enabled the information technology industry to double the performance and functionality of digital electronics roughly every 2 years within a fixed cost, power and area. Advances in silicon lithography have enabled this exponential miniaturization of electronics, but, as transistors reach atomic scale and fabrication costs continue to rise, the classical technological driver that has underpinned Moore's Law for 50 years is failing and is anticipated to flatten by 2025. This article provides an updated view of what a post-exascale system will look like and the challenges ahead, based on our most recent understanding of technology roadmaps. It also discusses the tapering of historical improvements, and how it affects options available to continue scaling of successors to the first exascale machine. Lastly, this article covers the many different opportunities and strategies available to continue computing performance improvements in the absence of historical technology drivers. This article is part of a discussion meeting issue 'Numerical algorithms for high-performance computational science'. }
}
@article{Sarvamangala2022,
  author   = {D. R. Sarvamangala and Raghavendra V. Kulkarni},
  title    = {Convolutional neural networks in medical image understanding: a survey},
  journal  = {Evolutionary Intelligence},
  year     = {2022},
  volume   = {15},
  number   = {1},
  pages    = {1--22},
  doi      = {10.1007/s12065-020-00540-3},
  url      = {https://doi.org/10.1007/s12065-020-00540-3},
  issn     = {1864-5917},
  abstract = {Imaging techniques are used to capture anomalies of the human body. The captured images must be understood for diagnosis, prognosis, and treatment planning of the anomalies. Medical image understanding is generally performed by skilled medical professionals. However, the scarce availability of human experts and the fatigue and rough estimate procedures involved with them limit the effectiveness of image understanding performed by skilled medical professionals. Convolutional neural networks (CNNs) are effective tools for image understanding. They have outperformed human experts in many image understanding tasks. This article aims to provide a comprehensive survey of applications of CNNs in medical image understanding. The underlying objective is to motivate medical image understanding researchers to extensively apply CNNs in their research and diagnosis. A brief introduction to CNNs has been presented. A discussion on CNN and its various award-winning frameworks have been presented. The major medical image understanding tasks, namely image classification, segmentation, localization, and detection have been introduced. Applications of CNN in medical image understanding of the ailments of brain, breast, lung, and other organs have been surveyed critically and comprehensively. A critical discussion on some of the challenges is also presented.}
}

@misc{WeiEtAl2023,
  title         = {Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation},
  author        = {Qingyue Wei and Lequan Yu and Xianhang Li and Wei Shao and Cihang Xie and Lei Xing and Yuyin Zhou},
  year          = {2023},
  eprint        = {2307.11604},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2307.11604}
}
@inproceedings{KimYujinAndLee2024,
  author    = {Kim, Yujin and Lee, Eunyeoul and Lee, Yunjung and Oh, Uran},
  title     = {Understanding Novice's Annotation Process For 3D Semantic Segmentation Task With Human-In-The-Loop},
  year      = {2024},
  isbn      = {9798400705083},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3640543.3645150},
  doi       = {10.1145/3640543.3645150},
  abstract  = {Large-scale 3D point clouds are often used as training data for 3D semantic segmentation, but the labor-intensive nature of the annotation process challenges the acquisition of sufficient labeled data. Meanwhile, there has been limited research on introducing novice annotators to acquire the labeled data by enhancing their annotation performance and user experience. Therefore, in this study, we explored solutions involving two dimensions: the presence of AI assistance and the number of classes visualized simultaneously in model's segmentation results in HITL. We conducted a user study with 16 novice annotators who had no prior experience in 3D semantic segmentation, asking them to perform annotation tasks. The results revealed an interaction effect between the two dimensions on annotation accuracy and labeling efficiency. We also found that displaying multiple classes at once reduced the time taken for annotation. Moreover, visualizing multiple classes at once or the absence of AI assistance led to a greater increase in model accuracy compared to our baselines. The best user experience was observed when the visualization showed a single class at a time with AI assistance. Based on these findings, we discuss which environments can enhance novice annotators' annotation performance and user experience in 3D semantic segmentation tasks within HITL contexts.},
  booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages     = {444–454},
  numpages  = {11},
  keywords  = {3D point cloud, AI assistance, Human-in-the-loop, Novice annotator, Visualization method},
  location  = {Greenville, SC, USA},
  series    = {IUI '24}
}
 @article{Karthikeyan2023,
  title     = {What's in a Label? Annotation Differences in Forecasting Mental Fatigue using ECG Data and Seq2Seq Architectures},
  url       = {http://dx.doi.org/10.36227/techrxiv.24501295.v1},
  doi       = {10.36227/techrxiv.24501295.v1},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Karthikeyan, Rohith and McDonald, Anthony and Mehta, Ranjana},
  year      = {2023},
  month     = nov
}

@article{EramianEtAl2020,
  author   = {Eramian, Mark and Power, Christopher and Rau, Stephen and Khandelwal, Pulkit},
  title    = {Benchmarking Human Performance in Semi-Automated Image Segmentation},
  journal  = {Interacting with Computers},
  volume   = {32},
  number   = {3},
  pages    = {233-245},
  year     = {2020},
  month    = {08},
  abstract = {Semi-automated segmentation algorithms hold promise for improving extraction and identification of objects in images such as tumors in medical images of human tissue, counting plants or flowers for crop yield prediction or other tasks where object numbers and appearance vary from image to image. By blending markup from human annotators to algorithmic classifiers, the accuracy and reproducability of image segmentation can be raised to very high levels. At least, that is the promise of this approach, but the reality is less than clear. In this paper, we review the state-of-the-art in semi-automated image segmentation performance assessment and demonstrate it to be lacking the level of experimental rigour needed to ensure that claims about algorithm accuracy and reproducability can be considered valid. We follow this review with two experiments that vary the type of markup that annotators make on images, either points or strokes, in tightly controlled experimental conditions in order to investigate the effect that this one particular source of variation has on the accuracy of these types of systems. In both experiments, we found that accuracy substantially increases when participants use a stroke-based interaction. In light of these results, the validity of claims about algorithm performance are brought into sharp focus, and we reflect on the need for a far more control on variables for benchmarking the impact of annotators and their context on these types of systems.},
  issn     = {1873-7951},
  doi      = {10.1093/iwcomp/iwaa017},
  url      = {https://doi.org/10.1093/iwcomp/iwaa017},
  eprint   = {https://academic.oup.com/iwc/article-pdf/32/3/233/38855643/iwaa017.pdf}
}


@article{Giri_Bhatia_2024,
  title        = {Artificial Intelligence in Nephrology- Its Applications from Bench to Bedside},
  volume       = {7},
  url          = {https://journalijanr.com/index.php/IJANR/article/view/60},
  abstractnote = {&amp;lt;p&amp;gt;Artificial intelligence (AI) is a science of computer stimulated thinking processes and human behaviours, which involves computer science, psychology, philosophy and linguistics. AI helps to access large patients' datasets which it can acquire and store for further processing. Computer aided diagnosis (CAD) combines both medical image and computer image processing to characterise the pathology accurately as well as promptly. The utility of CAD has been evident in many clinical scenarios especially in skin cancer, breast cancer and lung cancer. AI helps to retrieve data on treatment protocols from various clinical trials, develop models based on treatment efficacy, predicts the best treatment protocols and improves patient management.&amp;amp;nbsp; Furthermore, AI has significant impact in the branch of nephrology, where it aids in alerting acute kidney injury (AKI), predicting chronic kidney disease (CKD), accelerating diagnosis and guiding management. While AI medicine is still in its nascent stage, in the future, AI will play a crucial role in clinical practice.&amp;lt;/p&amp;gt;},
  number       = {1},
  journal      = {International Journal of Advances in Nephrology Research},
  author       = {Giri, Kajaree and Bhatia, Shailesh},
  year         = {2024},
  month        = {Aug.},
  pages        = {90–97}
}


@misc{YujiaEtAl2024,
  title         = {BCDNet: A Fast Residual Neural Network For Invasive Ductal Carcinoma Detection},
  author        = {Yujia Lin and Aiwei Lian and Mingyu Liao and Shuangjie Yuan},
  year          = {2024},
  eprint        = {2408.13800},
  archiveprefix = {arXiv},
  primaryclass  = {eess.IV},
  url           = {https://arxiv.org/abs/2408.13800}
}

@article{Bilal2025,
  author  = {Bilal, Anas and Alkhathlan, Ali and Kateb, Faris A. and Tahir, Alishba and Shafiq, Muhammad and Long, Haixia},
  title   = {A quantum-optimized approach for breast cancer detection using SqueezeNet-SVM},
  journal = {Scientific Reports},
  year    = {2025},
  volume  = {15},
  number  = {1},
  pages   = {3254},
  doi     = {10.1038/s41598-025-86671-y},
  url     = {https://doi.org/10.1038/s41598-025-86671-y},
  issn    = {2045-2322}
}


@phdthesis{Habis2024,
  title       = {{Developing interactive artificial intelligence tools to assist pathologists with histology annotation}},
  author      = {Habis, Antoine Aur{\'e}lien},
  url         = {https://theses.hal.science/tel-04712685},
  number      = {2024IPPAT022},
  school      = {{Institut Polytechnique de Paris}},
  year        = {2024},
  month       = Jun,
  keywords    = {Deep Learning ; Histopathology ; Biomedical imaging ; Apprentissage profond ; Histopathologie ; Imagerie biom{\'e}dicale},
  type        = {Theses},
  pdf         = {https://theses.hal.science/tel-04712685v1/file/133975_HABIS_2024_archivage.pdf},
  hal_id      = {tel-04712685},
  hal_version = {v1}
}

@article{BanerjeeEtAl2025,
  author  = {Banerjee, Abhirup  and Shan, Hongming  and Feng, Ruibin },
  title   = {Editorial: Artificial intelligence applications for cancer diagnosis in radiology},
  journal = {Frontiers in Radiology},
  volume  = {5},
  year    = {2025},
  url     = {https://www.frontiersin.org/journals/radiology/articles/10.3389/fradi.2025.1493783},
  doi     = {10.3389/fradi.2025.1493783},
  issn    = {2673-8740}
}
@article{LopezEtAl2024,
  title    = {Learning from crowds for automated histopathological image segmentation},
  journal  = {Computerized Medical Imaging and Graphics},
  volume   = {112},
  pages    = {102327},
  year     = {2024},
  issn     = {0895-6111},
  doi      = {https://doi.org/10.1016/j.compmedimag.2024.102327},
  url      = {https://www.sciencedirect.com/science/article/pii/S0895611124000041},
  author   = {Miguel López-Pérez and Pablo Morales-Álvarez and Lee A.D. Cooper and Christopher Felicelli and Jeffery Goldstein and Brian Vadasz and Rafael Molina and Aggelos K. Katsaggelos},
  keywords = {Segmentation, Histopathology, Crowdsourcing, Cancer, Noisy labels},
  abstract = {Automated semantic segmentation of histopathological images is an essential task in Computational Pathology (CPATH). The main limitation of Deep Learning (DL) to address this task is the scarcity of expert annotations. Crowdsourcing (CR) has emerged as a promising solution to reduce the individual (expert) annotation cost by distributing the labeling effort among a group of (non-expert) annotators. Extracting knowledge in this scenario is challenging, as it involves noisy annotations. Jointly learning the underlying (expert) segmentation and the annotators' expertise is currently a commonly used approach. Unfortunately, this approach is frequently carried out by learning a different neural network for each annotator, which scales poorly when the number of annotators grows. For this reason, this strategy cannot be easily applied to real-world CPATH segmentation. This paper proposes a new family of methods for CR segmentation of histopathological images. Our approach consists of two coupled networks: a segmentation network (for learning the expert segmentation) and an annotator network (for learning the annotators' expertise). We propose to estimate the annotators' behavior with only one network that receives the annotator ID as input, achieving scalability on the number of annotators. Our family is composed of three different models for the annotator network. Within this family, we propose a novel modeling of the annotator network in the CR segmentation literature, which considers the global features of the image. We validate our methods on a real-world dataset of Triple Negative Breast Cancer images labeled by several medical students. Our new CR modeling achieves a Dice coefficient of 0.7827, outperforming the well-known STAPLE (0.7039) and being competitive with the supervised method with expert labels (0.7723). The code is available at https://github.com/wizmik12/CRowd_Seg.}
}

@article{Rashmi2021,
  author  = {Rashmi, R. and Prasad, Keerthana and Udupa, Chethana Babu K.},
  title   = {Breast histopathological image analysis using image processing techniques for diagnostic purposes: A methodological review},
  journal = {Journal of Medical Systems},
  volume  = {46},
  number  = {1},
  pages   = {7},
  year    = {2021},
  month   = {Dec},
  doi     = {10.1007/s10916-021-01786-9},
  url     = {https://doi.org/10.1007/s10916-021-01786-9},
  issn    = {1573-689X}
}

@article{Carmo2025,
  author  = {Carmo, Diedre S. and Pezzulo, Alejandro A. and Villacreses, Raul A. and Eisenbeisz, McKenna L. and Anderson, Rachel L. and Van Dorin, Sarah E. and Rittner, Letícia and Lotufo, Roberto A. and Gerard, Sarah E. and Reinhardt, Joseph M. and Comellas, Alejandro P.},
  title   = {Manual segmentation of opacities and consolidations on CT of long COVID patients from multiple annotators},
  journal = {Scientific Data},
  volume  = {12},
  number  = {1},
  pages   = {402},
  year    = {2025},
  month   = {March},
  doi     = {10.1038/s41597-025-04709-2},
  url     = {https://doi.org/10.1038/s41597-025-04709-2},
  issn    = {2052-4463}
}

@article{ZhouEtAl2024,
  author   = {Zhou, Zhongxing and Gong, Hao and Hsieh, Scott and McCollough, Cynthia H. and Yu, Lifeng},
  title    = {Image quality evaluation in deep-learning-based CT noise reduction using virtual imaging trial methods: Contrast-dependent spatial resolution},
  journal  = {Medical Physics},
  volume   = {51},
  number   = {8},
  pages    = {5399-5413},
  keywords = {computed tomography (CT), deep convolutional neural network (DCNN), deep learning image reconstruction and noise reduction (DLIR), image quality, virtual imaging trial},
  doi      = {https://doi.org/10.1002/mp.17029},
  url      = {https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.17029},
  eprint   = {https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.17029},
  abstract = {Abstract Background Deep-learning-based image reconstruction and noise reduction methods (DLIR) have been increasingly deployed in clinical CT. Accurate image quality assessment of these methods is challenging as the performance measured using physical phantoms may not represent the true performance of DLIR in patients since DLIR is trained mostly on patient images. Purpose In this work, we aim to develop a patient-data-based virtual imaging trial framework and, as a first application, use it to measure the spatial resolution properties of a DLIR method. Methods The patient-data-based virtual imaging trial framework consists of five steps: (1) insertion of lesions into projection domain data using the acquisition geometry of the patient exam to simulate different lesion characteristics; (2) insertion of noise into projection domain data using a realistic photon statistical model of the CT system to simulate different dose levels; (3) creation of DLIR-processed images from projection or image data; (4) creation of ensembles of DLIR-processed patient images from a large number of noise and lesion realizations; and (5) evaluation of image quality using ensemble DLIR images. This framework was applied to measure the spatial resolution of a ResNet based deep convolutional neural network (DCNN) trained on patient images. Lesions in a cylindrical shape and different contrast levels (−500, −100, −50, −20, −10 HU) were inserted to the lower right lobe of the liver in a patient case. Multiple dose levels were simulated (50\%, 25\%, 12.5\%). Each lesion and dose condition had 600 noise realizations. Multiple reconstruction and denoising methods were used on all the noise realizations, including the original filtered-backprojection (FBP), iterative reconstruction (IR), and the DCNN method with three different strength setting (DCNN-weak, DCNN-medium, and DCNN-strong). Mean lesion signal was calculated by performing ensemble averaging of all the noise realizations for each lesion and dose condition and then subtracting the lesion-present images from the lesion absent images. Modulation transfer functions (MTFs) both in-plane and along the z-axis were calculated based on the mean lesion signals. The standard deviations of MTFs at each condition were estimated with bootstrapping: randomly sampling (with replacement) all the DLIR/FBP/IR images from the ensemble data (600 samples) at each condition. The impact of varying lesion contrast, dose levels, and denoising strengths were evaluated. Statistical analysis with paired t-test was used to compare the z-axis and in-plane spatial resolution of five algorithms for five different contrasts and three dose levels. Results The in-plane and z-axis spatial resolution degradation of DCNN becomes more severe as the contrast or radiation dose decreased, or DCNN denoising strength increased. In comparison with FBP, a 59.5\% and 4.1\% reduction of in-plane and z-axis MTF (in terms of spatial frequencies at 50\% MTF), respectively, was observed at low contrast (−10 HU) for DCNN with the highest denoising strength at 25\% routine dose level. When the dose level reduces from 50\% to 12.5\% of routine dose, the in-plane and z-axis MTFs reduces from 92.1\% to 76.3\%, and from 98.9\% to 95.5\%, respectively, at contrast of −100 HU, using FBP as the reference. For most conditions of contrasts and dose levels, significant differences were found among the five algorithms, with the following relationship in both in-plane and cross-plane spatial resolution: FBP > DCNN-Weak > IR > DCNN-Medium > DCNN-Strong. The spatial resolution difference among algorithms decreases at higher contrast or dose levels. Conclusions A patient-data-based virtual imaging trial framework was developed and applied to measuring the spatial resolution properties of a DCNN noise reduction method at different contrast and dose levels using real patient data. As with other non-linear image reconstruction and post-processing techniques, the evaluated DCNN method degraded the in-plane and z-axis spatial resolution at lower contrast levels, lower radiation dose, and higher denoising strength.},
  year     = {2024}
}

@inproceedings{DuqueEtCardenasEtGil2024,
  author    = {Valencia-Duque, Andr{\'e}s Felipe and Cardenas-Pe{\~n}a, David Augusto and Gil-Gonz{\'a}lez, Juli{\'a}n and Orozco-Guti{\'e}rrez, {\'A}lvaro and Daza-Santacoloma, Genaro},
  booktitle = {2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  title     = {Breast Cancer Tissue Classification from Multiple Annotators using Chained Deep Learning Approaches},
  year      = {2024},
  pages     = {1-4},
  keywords  = {Deep learning;Training;Uncertainty;Prevention and mitigation;Gaussian processes;Probabilistic logic;Breast cancer;Data models;Medical diagnostic imaging;Immune system;Crowdsourcing;digital pathology;breast cancer;multiple annotator model;noise mitigation},
  doi       = {10.1109/EMBC53108.2024.10782427}
}

@article{GilGonzalesEtAl2025,
  title    = {Generalized cross-entropy for learning from crowds based on correlated chained Gaussian processes},
  journal  = {Results in Engineering},
  volume   = {25},
  pages    = {103863},
  year     = {2025},
  issn     = {2590-1230},
  doi      = {https://doi.org/10.1016/j.rineng.2024.103863},
  url      = {https://www.sciencedirect.com/science/article/pii/S2590123024021066},
  author   = {J. Gil-González and G. Daza-Santacoloma and D. Cárdenas-Peña and A. Orozco-Gutiérrez and A. Álvarez-Meza},
  keywords = {Learning from crowds, Gaussian processes, Bayesian framework, Noisy labels, Non-stationary data, Generalized cross-entropy},
  abstract = {Machine learning applications heavily depend on labeled data provided by domain experts to train accurate models. However, the cost and time constraints associated with expert labeling often make obtaining ground truth labels impractical. Crowdsourcing offers a cost-effective alternative for collecting annotations but introduces challenges such as varying label quality and noisy data. To address these issues, the field of learning from crowds has emerged, enabling supervised learning directly from crowdsourced data. Still, traditional approaches often fall short by assuming homogeneous behavior across the input feature space and imposing independence constraints on outputs. We introduced the correlated chained Gaussian process with enhanced generalized cross-entropy loss, termed CGP-GCE, which effectively models multiple annotators' non-stationary and interdependent behaviors by a Bayesian approach. Besides, our proposal achieves a suitable trade-off between mean absolute error and cross-entropy function, significantly mitigating the impact of noisy labels and enhancing the robustness and accuracy of classification tasks in crowdsourced data environments. Through extensive simulated and real-world experiments, CCGP-GCE demonstrated superior classification performance, outperforming state-of-the-art multilabeler models in terms of both accuracy and expert reliability estimation. We aim to extend CCGP-GCE for future work to handle sparse and imbalanced annotations. Additionally, we plan to apply this model to multimodal tasks.}
}
@article{TrianaEtAl2023,
  author         = {Triana-Martinez, Jenniffer Carolina and Gil-González, Julian and Fernandez-Gallego, Jose A. and Álvarez-Meza, Andrés Marino and Castellanos-Dominguez, Cesar German},
  title          = {Chained Deep Learning Using Generalized Cross-Entropy for Multiple Annotators Classification},
  journal        = {Sensors},
  volume         = {23},
  year           = {2023},
  number         = {7},
  article-number = {3518},
  url            = {https://www.mdpi.com/1424-8220/23/7/3518},
  pubmedid       = {37050578},
  issn           = {1424-8220},
  abstract       = {Supervised learning requires the accurate labeling of instances, usually provided by an expert. Crowdsourcing platforms offer a practical and cost-effective alternative for large datasets when individual annotation is impractical. In addition, these platforms gather labels from multiple labelers. Still, traditional multiple-annotator methods must account for the varying levels of expertise and the noise introduced by unreliable outputs, resulting in decreased performance. In addition, they assume a homogeneous behavior of the labelers across the input feature space, and independence constraints are imposed on outputs. We propose a Generalized Cross-Entropy-based framework using Chained Deep Learning (GCECDL) to code each annotator's non-stationary patterns regarding the input space while preserving the inter-dependencies among experts through a chained deep learning approach. Experimental results devoted to multiple-annotator classification tasks on several well-known datasets demonstrate that our GCECDL can achieve robust predictive properties, outperforming state-of-the-art algorithms by combining the power of deep learning with a noise-robust loss function to deal with noisy labels. Moreover, network self-regularization is achieved by estimating each labeler's reliability within the chained approach. Lastly, visual inspection and relevance analysis experiments are conducted to reveal the non-stationary coding of our method. In a nutshell, GCEDL weights reliable labelers as a function of each input sample and achieves suitable discrimination performance with preserved interpretability regarding each annotator's trustworthiness estimation.},
  doi            = {10.3390/s23073518}
}
@article{LamAndSuen1997,
  author   = {Lam, L. and Suen, S.Y.},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  title    = {Application of majority voting to pattern recognition: an analysis of its behavior and performance},
  year     = {1997},
  volume   = {27},
  number   = {5},
  pages    = {553-568},
  keywords = {Voting;Pattern recognition;Pattern analysis;Performance analysis;Character recognition;Bayesian methods;Educational programs;Neural networks;Databases;Solids},
  doi      = {10.1109/3468.618255}
}


@inproceedings{TianEtZhu2015,
  author    = {TIAN, TIAN and Zhu, Jun},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Max-Margin Majority Voting for Learning from Crowds},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},
  volume    = {28},
  year      = {2015}
}

@inproceedings{ImbajoaRuiz2016,
  title     = {Multi-labeler Classification Using Kernel Representations and Mixture of Classifiers},
  author    = {David Esteban Imbajoa-Ruiz and I. D. Gustin and M. Bola{\~n}os-Ledezma and Andr{\'e}s F. Arciniegas-Mej{\'i}a and F. A. Guasmayan-Guasmayan and M. J. Bravo-Montenegro and Andr{\'e}s Eduardo Castro-Ospina and Diego Hern{\'a}n Peluffo-Ord{\'o}{\~n}ez},
  booktitle = {Iberoamerican Congress on Pattern Recognition},
  year      = {2016},
  url       = {https://api.semanticscholar.org/CorpusID:4963948}
}

@article{Elnakib2020,
  title     = {Automated deep system for joint liver and tumor segmentation using majority voting},
  author    = {Elnakib, Ahmed and Elmenabawy, Nermeen and S Moustafa, H},
  journal   = {MEJ-Mansoura Engineering Journal},
  volume    = {45},
  number    = {4},
  pages     = {30--36},
  year      = {2020},
  publisher = {Mansoura University, Faculty of Engineering}
}

@article{WarfieldEtAl2004,
  author   = {Warfield, S.K. and Zou, K.H. and Wells, W.M.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation},
  year     = {2004},
  volume   = {23},
  number   = {7},
  pages    = {903-921},
  keywords = {Image segmentation;Biomedical imaging;Humans;Radiology;Hospitals;Imaging phantoms;Performance analysis;Computer science;Artificial intelligence;Pathology},
  doi      = {10.1109/TMI.2004.828354}
}

@misc{IbrahimEtAl2024,
  title         = {Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective},
  author        = {Shahana Ibrahim and Panagiotis A. Traganitis and Xiao Fu and Georgios B. Giannakis},
  year          = {2024},
  eprint        = {2407.06902},
  archiveprefix = {arXiv},
  primaryclass  = {eess.SP},
  url           = {https://arxiv.org/abs/2407.06902}
}
@article{GrefveEtAl2024,
  title    = {Histopathology-validated gross tumor volume delineations of intraprostatic lesions using PSMA-positron emission tomography/multiparametric magnetic resonance imaging},
  journal  = {Physics and Imaging in Radiation Oncology},
  volume   = {31},
  pages    = {100633},
  year     = {2024},
  issn     = {2405-6316},
  doi      = {https://doi.org/10.1016/j.phro.2024.100633},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405631624001039},
  author   = {Josefine Grefve and Karin Söderkvist and Adalsteinn Gunnlaugsson and Kristina Sandgren and Joakim Jonsson and Angsana {Keeratijarut Lindberg} and Erik Nilsson and Jan Axelsson and Anders Bergh and Björn Zackrisson and Mathieu Moreau and Camilla {Thellenberg Karlsson} and Lars.E. Olsson and Anders Widmark and Katrine Riklund and Lennart Blomqvist and Vibeke {Berg Loegager} and Sara N. Strandberg and Tufve Nyholm},
  abstract = {Background and purpose
              Dose escalation in external radiotherapy of prostate cancer shows promising results in terms of biochemical disease-free survival. Boost volume delineation guidelines are sparse which may cause high interobserver variability. The aim of this research was to characterize gross tumor volume (GTV) delineations based on multiparametric magnetic resonance imaging (mpMRI) and prostate specific membrane antigen-positron emission tomography (PSMA-PET) in relation to histopathology-validated Gleason grade 4 and 5 regions.
              Material and methods
              The study participants were examined with [68Ga]PSMA-PET/mpMRI prior to radical prostatectomy. Four radiation oncologists delineated GTVs in 15 study participants, on four different image types; T2-weighted (T2w), diffusion weighted imaging (DWI), dynamic contrast enhanced (DCE) and PSMA-PET scans separately. The simultaneous truth and performance level estimation (STAPLE) algorithm was used to generate combined GTVs. GTVs were subsequently compared to histopathology. We analysed how Dice similarity coefficient (DSC) and lesion coverage are affected by using single versus multiple image types as well as by adding a clinical target volume (CTV) margin.
              Results
              Median DSC (STAPLE) for different GTVs varied between 0.33 and 0.52. GTVPSMA-PET/mpMRI generated the highest median lesion coverage at 0.66. Combining different image types achieved similar lesion coverage as adding a CTV margin to contours from a single image type, while reducing non-malignant tissue inclusion within the target volume.
              Conclusion
              The combined use of mpMRI or PSMA-PET/mpMRI shows promise, achieving higher DSC and lesion coverage while minimizing non-malignant tissue inclusion, in comparison to the use of a single image type with an added CTV margin.}
}
@article{QiuEtAl2022,
  author   = {Qiu, Yali  and Hu, Yujin  and Kong, Peiyao  and Xie, Hai  and Zhang, Xiaoliu  and Cao, Jiuwen  and Wang, Tianfu  and Lei, Baiying },
  title    = {Automatic Prostate Gleason Grading Using Pyramid Semantic Parsing Network in Digital Histopathology},
  journal  = {Frontiers in Oncology},
  volume   = {12},
  year     = {2022},
  url      = {https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2022.772403},
  doi      = {10.3389/fonc.2022.772403},
  issn     = {2234-943X},
  abstract = {<sec><title>Purpose</title><p>Prostate biopsy histopathology and immunohistochemistry are important in the differential diagnosis of the disease and can be used to assess the degree of prostate cancer differentiation. Today, prostate biopsy is increasing the demand for experienced uropathologists, which puts a lot of pressure on pathologists. In addition, the grades of different observations had an indicating effect on the treatment of the patients with cancer, but the grades were highly changeable, and excessive treatment and insufficient treatment often occurred. To alleviate these problems, an artificial intelligence system with clinically acceptable prostate cancer detection and Gleason grade accuracy was developed.</p></sec><sec><title>Methods</title><p>Deep learning algorithms have been proved to outperform other algorithms in the analysis of large data and show great potential with respect to the analysis of pathological sections. Inspired by the classical semantic segmentation network, we propose a pyramid semantic parsing network (PSPNet) for automatic prostate Gleason grading. To boost the segmentation performance, we get an auxiliary prediction output, which is mainly the optimization of auxiliary objective function in the process of network training. The network not only includes effective global prior representations but also achieves good results in tissue micro-array (TMA) image segmentation.</p></sec><sec><title>Results</title><p>Our method is validated using 321 biopsies from the Vancouver Prostate Centre and ranks the first on the MICCAI 2019 prostate segmentation and classification benchmark and the Vancouver Prostate Centre data. To prove the reliability of the proposed method, we also conduct an experiment to test the consistency with the diagnosis of pathologists. It demonstrates that the well-designed method in our study can achieve good results. The experiment also focused on the distinction between high-risk cancer (Gleason pattern 4, 5) and low-risk cancer (Gleason pattern 3). Our proposed method also achieves the best performance with respect to various evaluation metrics for distinguishing benign from malignant.</p></sec><sec><title>Availability</title><p>The Python source code of the proposed method is publicly available at <uri xlink:href="https://github.com/hubutui/Gleason" xmlns:xlink="http://www.w3.org/1999/xlink">https://github.com/hubutui/Gleason</uri>. All implementation details are presented in this paper.</p></sec><sec><title>Conclusion</title><p>These works prove that the Gleason grading results obtained from our method are effective and accurate.</p></sec>}
}

@inproceedings{RonnebergerEtAl2015,
  author    = {Ronneberger, Olaf
               and Fischer, Philipp
               and Brox, Thomas},
  editor    = {Navab, Nassir
               and Hornegger, Joachim
               and Wells, William M.
               and Frangi, Alejandro F.},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
  year      = {2015},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {234--241},
  abstract  = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn      = {978-3-319-24574-4}
}



@inproceedings{ZhangEtAl2020,
  author    = {Zhang, Le and Tanno, Ryutaro and Xu, Mou-Cheng and Jin, Chen and Jacob, Joseph and Cicarrelli, Olga and Barkhof, Frederik and Alexander, Daniel},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {15750--15762},
  publisher = {Curran Associates, Inc.},
  title     = {Disentangling Human Error from Ground Truth in Segmentation of Medical Images},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b5d17ed2b502da15aa727af0d51508d6-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@misc{GilEtAlvarez2023CCGPMA,
  author   = {Gil González Julián and Álvarez Meza Andrés Marino},
  title    = {A Supervised Learning Framework in the Context of Multiple Annotators},
  year     = {2023},
  abstract = {The increasing popularity of crowdsourcing platforms, i.e., Amazon Mechanical Turk, is changing how datasets for supervised learning are built. In these cases, instead of having datasets labeled by one source (which is supposed to be an expert who provided the absolute gold standard), we have datasets labeled by multiple annotators with different and unknown expertise. Hence, we face a multi-labeler scenario, which typical supervised learning models cannot tackle.For this reason, much attention has recently been given to the approaches that capture multiple annotators' wisdom. However, such methods reside on two key assumptions: the labeler's performance does not depend on the input space and independence among the annotators, which are hardly feasible in real-world settings. This book exploresseveral models based on both frequentist and Bayesian perspectives aiming to face multi-labeler scenarios. Our approaches model the annotators' behavior by considering the relationship between the input space and the labelers' performance and coding interdependencies among them.},
  url      = {https://repositorio.unal.edu.co/handle/unal/84685},
  chapter  = {5}
}

@misc{ZhangEtAl2018,
  title         = {Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels},
  author        = {Zhilu Zhang and Mert R. Sabuncu},
  year          = {2018},
  eprint        = {1805.07836},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1805.07836}
}

@article{HousseinEtAl2021,
  title    = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
  journal  = {Expert Systems with Applications},
  volume   = {167},
  pages    = {114161},
  year     = {2021},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2020.114161},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417420309015},
  author   = {Essam H. Houssein and Marwa M. Emam and Abdelmgeid A. Ali and Ponnuthurai Nagaratnam Suganthan},
  keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images},
  abstract = {Breast cancer is the second leading cause of death for women, so accurate early detection can help decrease breast cancer mortality rates. Computer-aided detection allows radiologists to detect abnormalities efficiently. Medical images are sources of information relevant to the detection and diagnosis of various diseases and abnormalities. Several modalities allow radiologists to study the internal structure, and these modalities have been met with great interest in several types of research. In some medical fields, each of these modalities is of considerable significance. This study aims at presenting a review that shows the new applications of machine learning and deep learning technology for detecting and classifying breast cancer and provides an overview of progress in this area. This review reflects on the classification of breast cancer utilizing multi-modalities medical imaging. Details are also given on techniques developed to facilitate the classification of tumors, non-tumors, and dense masses in various medical imaging modalities. It first provides an overview of the different approaches to machine learning, then an overview of the different deep learning techniques and specific architectures for the detection and classification of breast cancer. We also provide a brief overview of the different image modalities to give a complete overview of the area. In the same context, this review was performed using a broad variety of research databases as a source of information for access to various field publications. Finally, this review summarizes the future trends and challenges in the classification and detection of breast cancer.}
}
@article{PanEtAl2021,
  title    = {Mitosis detection techniques in H\&E stained breast cancer pathological images: A comprehensive review},
  journal  = {Computers \& Electrical Engineering},
  volume   = {91},
  pages    = {107038},
  year     = {2021},
  issn     = {0045-7906},
  doi      = {https://doi.org/10.1016/j.compeleceng.2021.107038},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045790621000586},
  author   = {Xipeng Pan and Yinghua Lu and Rushi Lan and Zhenbing Liu and Zujun Qin and Huadeng Wang and Zaiyi Liu},
  keywords = {Breast cancer, Mitosis, Detection, Machine learning, Deep learning},
  abstract = {Quantifying mitosis in pathological sections is of great significance in the pathological diagnosis of breast cancer as it is used to evaluate the aggressiveness of the tumor and to provide more comprehensive and reliable information for accurate diagnosis and treatment. In this paper, we summarized the current mainstream methods of mitosis detection and divided them into four categories, namely traditional methods, deep learning methods, methods combining deep learning with traditional methods and other methods. Each method is introduced, and the performance indicators achieved by some of these methods and their results are discussed, compared, and evaluated. We summarize some solutions to the problem of positive and negative sample imbalance in mitotic datasets. Through the review of the research methods in this field, the existing methods of mitosis research in breast cancer are summarized, and the future developments are prospected.}
}

@article{AmgadEtAl2019,
  author   = {Amgad, Mohamed and Elfandy, Habiba and Hussein, Hagar and Atteya, Lamees A and Elsebaie, Mai A T and Abo Elnasr, Lamia S and Sakr, Rokia A and Salem, Hazem S E and Ismail, Ahmed F and Saad, Anas M and Ahmed, Joumana and Elsebaie, Maha A T and Rahman, Mustafijur and Ruhban, Inas A and Elgazar, Nada M and Alagha, Yahya and Osman, Mohamed H and Alhusseiny, Ahmed M and Khalaf, Mariam M and Younes, Abo-Alela F and Abdulkarim, Ali and Younes, Duaa M and Gadallah, Ahmed M and Elkashash, Ahmad M and Fala, Salma Y and Zaki, Basma M and Beezley, Jonathan and Chittajallu, Deepak R and Manthey, David and Gutman, David A and Cooper, Lee A D},
  title    = {Structured crowdsourcing enables convolutional segmentation of histology images},
  journal  = {Bioinformatics},
  volume   = {35},
  number   = {18},
  pages    = {3461-3467},
  year     = {2019},
  month    = {02},
  abstract = {While deep-learning algorithms have demonstrated outstanding performance in semantic image segmentation tasks, large annotation datasets are needed to create accurate models. Annotation of histology images is challenging due to the effort and experience required to carefully delineate tissue structures, and difficulties related to sharing and markup of whole-slide images.We recruited 25 participants, ranging in experience from senior pathologists to medical students, to delineate tissue regions in 151 breast cancer slides using the Digital Slide Archive. Inter-participant discordance was systematically evaluated, revealing low discordance for tumor and stroma, and higher discordance for more subjectively defined or rare tissue classes. Feedback provided by senior participants enabled the generation and curation of 20 000+ annotated tissue regions. Fully convolutional networks trained using these annotations were highly accurate (mean AUC=0.945), and the scale of annotation data provided notable improvements in image classification accuracy.Dataset is freely available at: https://goo.gl/cNM4EL.Supplementary data are available at Bioinformatics online.},
  issn     = {1367-4803},
  doi      = {10.1093/bioinformatics/btz083},
  url      = {https://doi.org/10.1093/bioinformatics/btz083},
  eprint   = {https://academic.oup.com/bioinformatics/article-pdf/35/18/3461/48975491/bioinformatics\_35\_18\_3461.pdf}
}

@article{FarahaniEtAl2015,
  author  = {Farahani, Navid and Parwani, Anil V and Pantanowitz, Liron},
  title   = {Whole slide imaging in pathology: advantages, limitations, and emerging perspectives},
  journal = {Pathology and Laboratory Medicine International},
  volume  = {7},
  pages   = {23-33},
  year    = {2015},
  doi     = {10.2147/PLMI.S59826}
}



@article{MazzariniEtAl2021,
  author   = {Mazzarini, Maria and Falchi, Mario and Bani, Daniele and Migliaccio, Anna Rita},
  title    = {Evolution and new frontiers of histology in bio-medical research},
  journal  = {Microscopy Research and Technique},
  volume   = {84},
  number   = {2},
  pages    = {217-237},
  keywords = {digital imaging, dynamic microscopy, precision microscopy, quantitative microscopy},
  doi      = {https://doi.org/10.1002/jemt.23579},
  url      = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/jemt.23579},
  eprint   = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/pdf/10.1002/jemt.23579},
  abstract = {Abstract Histology refers to the study of the morphology of cells within their natural tissue environment. As a bio-medical discipline, it dates back to the development of first microscopes which allowed to override the physical visual limitation of the human eye. Since the first observations, it was understood that cell shape predicts function and, therefore, shape alterations can identify and explain dysfunction and diseases. The advancements in morphological investigation techniques have allowed to extend our understanding of the shape–function relationships close to the molecular level of organization of tissues, as well as to derive reliable data not only from fixed, and hence static, biological samples but also living cells and tissues and even for extended time periods. These modern approaches, which encompass quantitative microscopy, precision microscopy, and dynamic microscopy, represent the new frontier of morphology. This article summarizes how the microscopy techniques have evolved to properly face the challenges of biomedical sciences, thus transforming histology from a merely qualitative discipline, which played an ancillary role to traditional "major" sciences such as anatomy, to a modern experimental science capable of driving knowledge progress in biology and medicine.},
  year     = {2021}
}

@article{AlturkistaniEtAl2015,
  author   = {Alturkistani, Hani A. and Tashkandi, Fatima M. and Mohammedsaleh, Zuhair M.},
  title    = {Histological Stains: A Literature Review and Case Study},
  journal  = {Global Journal of Health Science},
  volume   = {8},
  number   = {3},
  pages    = {72-79},
  year     = {2015},
  month    = {Jun},
  doi      = {10.5539/gjhs.v8n3p72},
  pmid     = {26493433},
  pmcid    = {PMC4804027},
  abstract = {},
  issn     = {1916-9736}
}

@article{WeitzEtAl2023,
  author   = {Weitz, Philippe and Valkonen, Masi and Solorzano, Leslie and Carr, Circe and Kartasalo, Kimmo and Boissin, Constance and Koivukoski, Sonja and Kuusela, Aino and Rasic, Dusan and Feng, Yanbo and {Sinius Pouplier}, Sandra and Sharma, Abhinav and {Ledesma Eriksson}, Kajsa and Latonen, Leena and Laenkholm, Anne-Vibeke and Hartman, Johan and Ruusuvuori, Pekka and Rantalainen, Mattias},
  title    = {A Multi-Stain Breast Cancer Histological Whole-Slide-Image Data Set from Routine Diagnostics},
  journal  = {Scientific Data},
  volume   = {10},
  number   = {1},
  pages    = {562},
  year     = {2023},
  doi      = {10.1038/s41597-023-02422-6},
  url      = {https://doi.org/10.1038/s41597-023-02422-6},
  abstract = {The analysis of FFPE tissue sections stained with haematoxylin and eosin (H\&E) or immunohistochemistry (IHC) is essential for the pathologic assessment of surgically resected breast cancer specimens. IHC staining has been broadly adopted into diagnostic guidelines and routine workflows to assess the status of several established biomarkers, including ER, PGR, HER2 and KI67. Biomarker assessment can also be facilitated by computational pathology image analysis methods, which have made numerous substantial advances recently, often based on publicly available whole slide image (WSI) data sets. However, the field is still considerably limited by the sparsity of public data sets. In particular, there are no large, high quality publicly available data sets with WSIs of matching IHC and H\&E-stained tissue sections from the same tumour. Here, we publish the currently largest publicly available data set of WSIs of tissue sections from surgical resection specimens from female primary breast cancer patients with matched WSIs of corresponding H\&E and IHC-stained tissue, consisting of 4,212 WSIs from 1,153 patients.},
  issn     = {2052-4463}
}


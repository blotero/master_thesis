\chapter{Chained Gaussian Processes}\label{ch:chained_gps}

\section{Background and Related Methods}\label{sec:background}

In recent years, several approaches have been proposed to handle
supervised learning problems in the context of multiple annotators.
This section reviews key methods that form the foundation for
understanding chained Gaussian processes and their application to
multiple annotator scenarios in regression and classification tasks.

\subsection{Kernel Alignment-Based Annotator Relevance Analysis (KAAR)}

The \gls{KAAR} method, introduced in \cite{GilEtAlvarez2023CCGPMA},
addresses the challenge of estimating annotator expertise in
scenarios where the ground truth is unavailable. The key innovation
of \gls{KAAR} lies in its use of \gls{CKA} to measure the similarity
between input features and annotator labels.

Given a dataset with input features $\mat{X}$ and labels from
multiple annotators $\mat{Y}$, KAAR computes a kernel matrix
$\mat{K}_{\ve{\nu}}$ as a convex combination of $R$ basis kernels:

\begin{align}
  \mat{K}_{\ve{\nu}} = \sum_{r=1}^{R}\nu_r\mat{K}_{r},
\end{align}

where $\mat{K}_{r}$ is computed using a kernel function over the
input features, and $\nu_r$ represents the relevance weight for the
$r$-th annotator. The weights are optimized by maximizing the CKA
between $\mat{K}_{\ve{\nu}}$ and a target kernel $\mat{F}$ computed
from the input features:

\begin{align}
  \rho(\mat{K}_{\ve{\nu}},\mat{F}) =
  \frac{{\langle\mat{\bar{K}_{\ve{\nu}}},\mat{\bar{F}}\rangle_{\texttt{F}}}}{{{\left|\right|\mat{\bar{K}_{\ve{\nu}}}\left|\right|_{\texttt{F}}\left|\right|\mat{\bar{F}}\left|\right|_{\texttt{F}}}}},
\end{align}

A key advantage of KAAR is its ability to capture dependencies
between annotators through the kernel framework, without requiring
explicit parametric modeling of these relationships.

\subsection{Localized Kernel Alignment-Based Annotator Relevance
Analysis (LKAAR)}

LKAAR extends KAAR by introducing locality in the kernel alignment
framework. Rather than assuming constant annotator performance across
the input space, LKAAR models the annotator relevance as a function
of the input features. The kernel combination in LKAAR takes the form:

\begin{equation}
  \mat{K}_{\ve{q}} =\sum_{r=1}^{R}\mat{Q}_r\mat{K}_r\mat{Q}_r,
\end{equation}

where $\mat{Q}_r$ is a diagonal matrix whose elements represent the
local relevance of annotator $r$. These elements are computed as:

\begin{align}
  q_r(\ve{x}_n) =
  \begin{cases}
    \beta_0^{(r)} + \sum_{n^{\prime}=1}^{N}\beta_{n^{\prime}}^{(r)}
    \kappa_{\beta}(\ve{x}_n,\ve{x}_{n^{\prime}}), & \mbox{if } n\in N_r\\
    0, & \mbox{Otherwise }
  \end{cases}
\end{align}

This localized approach allows LKAAR to better model inconsistent
annotators whose performance varies across different regions of the input space.

\subsection{Regularized Chained Deep Neural Network (RCDNN)}

RCDNN represents a deep learning approach to multiple annotator
learning, inspired by the chained Gaussian processes model. The
method models both the ground truth estimation and annotator
reliability through a deep neural network architecture. The
likelihood function in RCDNN takes the form:

\begin{align}
  p(\ve{Y}|\bm{\theta}) = \prod_{n=1}^{N}\prod_{r\in R_n}
  \left(\prod_{k=1}^{K}\zeta_{n,k}^{\delta(y_n^{(r)}-k)}\right)^{\lambda_n^{(r)}}\left(\frac{1}{K}\right)^{1-\lambda_n^{(r)}},
\end{align}

where $\lambda_n^{(r)}$ represents the reliability of annotator $r$
for instance $n$, and $\zeta_{n,k}$ is the estimated probability of
class $k$ for instance $n$.

The RCDNN architecture is built upon a sophisticated multi-layer
structure that combines several key innovations in deep learning. At
its core, it employs multiple dense layers with carefully chosen
activation functions, each designed to capture different aspects of
the annotator-data relationship. To prevent overfitting, a
comprehensive regularization strategy is implemented, combining both
l1 and l2 norms with dropout mechanisms. The network architecture
splits into specialized branches: one dedicated to estimating the
ground truth labels, and another focused on modeling annotator
reliability. This dual-branch approach allows the network to
simultaneously learn both the true underlying patterns in the data
and the individual characteristics of each annotator. Furthermore,
the implementation of Monte Carlo dropout during prediction provides
a mechanism for uncertainty estimation, offering not just predictions
but also confidence measures for those predictions.

\subsection{Chained Gaussian Processes Model}

The Chained Gaussian Processes (CGP) model provides a probabilistic
framework for modeling multiple outputs through a chain of Gaussian
processes. In the context of multiple annotators, CGP can be used to
model both the ground truth and annotator behavior. The key idea is
to model a likelihood function with $J$ parameters as in
\cite{GiraldoEtAl2022}:

\begin{align}
  p(\ve{y}|\ve{X},\bm{\theta}) =
  \prod_{n=1}^{N}p(y_n|\theta_1(\ve{x}_n),\dots , \theta_J(\ve{x}_n)),
\end{align}

where each parameter $\theta_j(\ve{x})$ is modeled using a separate
Gaussian process. This framework provides remarkable flexibility in
modeling complex relationships within the data. The use of Gaussian
processes enables natural uncertainty quantification through
posterior distributions, allowing the model to express varying
degrees of confidence in its predictions. The framework inherently
handles missing data through its probabilistic nature, and the
automatic relevance determination property of GPs helps identify the
most important features for prediction.

The CGP framework becomes particularly powerful when integrated with
concepts from KAAR and LKAAR. By incorporating kernel alignment
principles, the model can make more informed choices about kernel
functions, better capturing the underlying structure of the data. The
framework can be extended to include input-dependent lengthscales,
allowing for varying degrees of smoothness across the input space.
Furthermore, through multi-output GP formulations, the model can
capture complex dependencies between different annotators, providing
a rich representation of how different experts' opinions relate to each other.

However, implementing CGP in practice presents several significant
challenges. The computational complexity of the model scales
cubically with the dataset size, making it potentially prohibitive
for large-scale applications. The need for approximate inference
techniques in non-Gaussian likelihood scenarios can introduce
additional complexity and potential approximation errors. When
dealing with high-dimensional inputs, the model may struggle with the
curse of dimensionality, requiring careful feature selection or
dimensionality reduction preprocessing. Additionally, the selection
of appropriate kernel functions remains a critical challenge, as it
significantly impacts the model's performance and its ability to
capture relevant patterns in the data.

\section{Tooling and Implementation}\label{sec:tooling}

The implementation of Chained Gaussian Processes for multiple
annotator learning requires robust and flexible software tools that
can handle both the probabilistic nature of Gaussian processes and
the complexity of deep architectures. In our implementation, we
leverage two key frameworks: GPflow and GPflux. These tools provide
the foundation for building scalable and efficient implementations of
our models while maintaining the theoretical rigor necessary for
proper uncertainty quantification.

The choice of these frameworks is motivated by several factors.
First, both are built on top of TensorFlow, providing access to
automatic differentiation and GPU acceleration capabilities. Second,
they offer a modular design that allows for easy extension and
modification, which is crucial when implementing novel model
architectures. Third, they provide robust implementations of various
Gaussian process models and inference methods, allowing us to focus
on the specific challenges of multiple annotator learning rather than
reimplementing basic GP functionality.

\subsection{GPflow Overview}

GPflow is a Python package for building Gaussian process models in
TensorFlow. Developed as a modern and scalable alternative to GPy, it
leverages TensorFlow's computational capabilities to provide
efficient implementation of Gaussian process models. The framework is
designed with four key principles in mind: ease of use, computational
efficiency, extensibility, and automatic differentiation. This work
was originally published in \cite{MatthewsEtAl2016}.

At its core, GPflow provides implementations of various Gaussian
process models including GPR (Gaussian Process Regression), GPMC
(Gaussian Process Monte Carlo), SGPR (Sparse Gaussian Process
Regression), and SGPMC (Sparse Gaussian Process Monte Carlo). These
implementations are built on top of TensorFlow's computational graph
framework, allowing for automatic differentiation and GPU
acceleration where available.

The framework's architecture is modular, with clear separation
between model components such as kernels, likelihoods, and
optimization routines. This modularity makes it straightforward to
extend existing models or create new ones. GPflow includes a
comprehensive suite of kernel functions, ranging from the basic RBF
(Radial Basis Function) kernel to more sophisticated ones like the
Spectral Mixture kernel.

One of GPflow's distinguishing features is its robust handling of
model parameters. It provides facilities for parameter transformation
(ensuring parameters remain in valid ranges during optimization),
prior specification, and flexible optimization strategies. The
framework also includes utilities for model evaluation and
prediction, with built-in support for uncertainty estimation.

\subsection{GPflux Framework}

GPflux extends GPflow to enable the construction and training of Deep
Gaussian Process (DGP) models using modern deep learning practices.
Built on top of both GPflow and TensorFlow, GPflux provides a
flexible framework for implementing deep probabilistic models that
combine the expressiveness of deep neural networks with the
uncertainty quantification capabilities of Gaussian processes. This
work was originally published in \cite{DutordoirEtAl2021}.

The framework implements Deep Gaussian Processes as a composition of
Gaussian process layers, where each layer transforms its inputs
through a Gaussian process mapping. This hierarchical structure
allows the model to capture complex, non-stationary patterns in data
while maintaining uncertainty estimates throughout the entire
processing pipeline. GPflux handles this complexity through a
carefully designed architecture that separates the concerns of model
specification, inference, and prediction.

A key innovation in GPflux is its integration with Keras-style
layers, allowing practitioners to combine deterministic neural
network layers with Gaussian process layers in a single model. This
hybrid approach enables the construction of models that can leverage
both the structural assumptions encoded in neural network
architectures and the flexibility of Gaussian processes.

For inference, GPflux implements various approximate methods,
including stochastic variational inference with inducing points,
which makes it possible to train deep Gaussian process models on
large datasets. The framework provides built-in support for minibatch
training, making it practical to work with datasets that don't fit in memory.

\subsection{Implementation Considerations}

When implementing Chained Gaussian Processes for multiple annotator
learning using these tools, several key considerations must be
addressed. First, the model architecture must be carefully designed
to capture both the individual annotator characteristics and their
interdependencies. This requires extending the basic GP layers
provided by GPflux to handle multiple output streams - one for each
annotator's reliability model.

Second, the inference procedure must be adapted to handle the unique
challenges of multiple annotator data, such as missing labels and
varying levels of annotator expertise across different regions of the
input space. The variational inference capabilities of GPflux provide
a solid foundation for this, but careful attention must be paid to
the design of the variational approximation to ensure it captures the
relevant uncertainty in both the ground truth and annotator
reliability estimates.

Finally, computational efficiency becomes a critical concern when
scaling to large datasets with multiple annotators. The minibatch
training capabilities of both frameworks help address this, but
additional considerations such as appropriate inducing point
selection and batch size tuning become important for achieving good
performance in practice.

\section{Experimental Setup}\label{sec:experiments}

Synthetic datasets were used to evaluate the proposed methods in
classification and regression tasks, following very similar
procedures to the ones used in \cite{GilGonzalezEtAl2023}, which
are as follows:

\subsection{Classification}

A fully synthetic data for a 1-D ($P=1$) multiclass classification
problem with $K=3$ classes is generated. The input feature matrix
$\mathbf{X}$ is constructed by randomly sampling $N=100$ points from
a uniform distribution over the interval $[0, 1]$. For each sample
$n$, the true label is determined by computing three target values:
\begin{align*}
  t_{n,1} &= \sin(2\pi x_n), \\
  t_{n,2} &= -\sin(2\pi x_n), \\
  t_{n,3} &= -\sin(2\pi (x_n + 0.25)) + 0.5,
\end{align*}
and assigning the class label as the index $i \in \{1, 2, 3\}$ that
maximizes $t_{n,i}$. This construction ensures a nontrivial,
nonlinear decision boundary between the classes.

For evaluation, the test set is generated by extracting 200 equally
spaced samples from the interval $[0, 1]$, providing a dense coverage
of the input space to assess generalization performance.

Note that at this point, the fully synthetic dataset does not
contain real annotator labels. Therefore, it is necessary to simulate
annotator labels as corrupted versions of the hidden ground truth. In
these experiments, the simulation is designed to account for: 1)
dependencies among annotators, and 2) labeler performance that varies
as a function of the input features.

To achieve this, an \gls{SLFM}-based approach (termed SLFM-C) is employed,
which generates annotator labels as follows:

\begin{enumerate}
\item \textbf{Definition of deterministic functions:} Define $Q$
deterministic functions $\hat{\mu}_q : \mathcal{X} \rightarrow
\mathbb{R}$ and their combination parameters $\hat{w}_{l,r,q} \in
\mathbb{R}$, for all $r \in R$, $n \in N$.
\item \textbf{Annotator performance computation:} Compute
$\hat{f}_{l,r,n} = \sum_{q=1}^{Q} \hat{w}_{l,r,q}
\hat{\mu}_q(\hat{x}_n)$, where $\hat{x}_n \in \mathbb{R}$ is the
$n$th component of $\hat{\mathbf{x}} \in \mathbb{R}^N$. Here,
$\hat{\mathbf{x}}$ is the 1-D representation of the input features in
$\mathbf{X}$, obtained using the t-distributed Stochastic Neighbor
Embedding (t-SNE) approach.
\item \textbf{Reliability calculation:} Calculate $\hat{\lambda}^r_n
= \varsigma(\hat{f}_{l,r,n})$, where $\varsigma(\cdot) \in [0,1]$ is
the sigmoid function.
\item \textbf{Label assignment:} The $r$th annotator's label for the
$n$th sample is given by
\[
  y^r_n =
  \begin{cases}
    y_n, & \text{if } \hat{\lambda}^r_n \geq 0.5 \\
    \tilde{y}_n, & \text{if } \hat{\lambda}^r_n < 0.5
  \end{cases}
\]
where $y_n$ is the true label and $\tilde{y}_n$ is a flipped version
of the actual label $y_n$.
\end{enumerate}

This procedure ensures that the simulated annotator labels exhibit
both inter-annotator dependencies and input-dependent reliability.

\subsection{Regression}

A fully synthetic dataset is also generated for a 1-D regression problem,
where the ground truth for the $n$th sample is defined as:

\[
y_n = \sin(2\pi x_n) \sin(6\pi x_n)
\]

Here, the input matrix $\mathbf{X}$ is constructed by randomly
sampling 100 points uniformly from the interval $[0, 1]$. The test
set is created by extracting equally spaced samples from the same
interval, ensuring a clear separation between training and testing data.

To simulate the presence of multiple annotators, the ground truth is
corrupted with annotator-specific noise. For each annotator $r$, the
observed label for the $n$th sample is given by:

\[
y_n^{r} = y_n + \epsilon_n^{r}
\]

where $\epsilon_n^{r} \sim \mathcal{N}(0, v_n^{r})$ is Gaussian
noise, and $v_n^{r}$ represents the error variance for the $r$th
annotator, which may depend on the input features. This setup allows
us to model both the bias and the reliability of each annotator, as
well as potential dependencies between annotators.

To further increase realism, the error variance $v_n^{r}$ can be
modeled as a function of the input features, capturing scenarios
where annotator reliability varies across the input space, thus:

\[
\epsilon_n^r \sim \mathcal{N}(0, v_n^r)
\]

being $v_n^r$, the $r$th annotator's error variance for the $n$th sample.

Again, for modeling the error as a function of the input features,
an \gls{SLFM}-based approach (termed SLFM-R) is employed to build the
noisy labels, as follows:

\begin{enumerate}
\item \textbf{Definition of deterministic functions:} Define $Q$
deterministic functions $\hat{\mu}_q : \mathcal{X} \rightarrow
\mathbb{R}$ and their combination parameters $\hat{w}_{l,r,q} \in
\mathbb{R}$, for all $r \in R$, $n \in N$.
\item \textbf{Annotator performance computation:} Compute
$\hat{f}_{l,r,n} = \sum_{q=1}^{Q} \hat{w}_{l,r,q}
\hat{\mu}_q(\hat{x}_n)$, where $\hat{x}_n \in \mathbb{R}$ is the
$n$th component of $\hat{\mathbf{x}} \in \mathbb{R}^N$. Here,
$\hat{\mathbf{x}}$ is the 1-D representation of the input features in
$\mathbf{X}$, obtained using the t-distributed Stochastic Neighbor
Embedding (t-SNE) approach.
\item \textbf{Compute variance :} Finally, determine
$v_n^r=\exp(\hat{f}_{l,r,n})$.
\end{enumerate}

\section{Summary}\label{sec:summary}
\subsection{Comparative Analysis}

The evolution of multiple annotator learning methods represents a
fascinating progression in how we approach the challenge of learning
from diverse expert opinions. KAAR established a foundation with its
non-parametric approach through kernel alignment, though it operates
under the assumption of constant annotator performance across the
input space. LKAAR advanced this concept by introducing
input-dependent annotator behavior, recognizing that expert
performance often varies across different types of inputs. RCDNN
brought the power and flexibility of deep learning to the problem,
offering excellent scalability but requiring careful attention to
regularization and architecture design. CGP represents perhaps the
most theoretically elegant approach, providing a principled
probabilistic framework, though it faces significant computational
challenges in practical applications.

The selection of an appropriate method for a given application
requires careful consideration of several factors. The size and
dimensionality of the dataset play a crucial role, as some methods
scale better than others with increasing data volume. The need for
uncertainty quantification may favor probabilistic approaches like
CGP, while computational constraints might push towards more
efficient methods like KAAR. The presence of input-dependent
annotator behavior could make LKAAR or RCDNN more appropriate
choices. Additionally, when interpretability is a key requirement, as
in many medical applications, methods like KAAR and LKAAR might be
preferred over the more complex deep learning approaches.
Table \ref{tab:methods_comparison} provides a comprehensive
comparison of the key characteristics and capabilities of each method discussed.

\begin{table}[!htb]
\caption{Comparison of Multiple Annotator Learning Methods}
\label{tab:methods_comparison}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}\toprule
  \textbf{Characteristic} & \textbf{KAAR} & \textbf{LKAAR} &
  \textbf{RCDNN} & \textbf{CGP} \\\midrule
  Input-dependent annotator behavior & No & Yes & Yes & Yes \\
  Annotator dependencies modeling & Yes & Yes & Yes & Yes \\
  Uncertainty quantification & No & No & Partial & Yes \\
  Computational scalability & High & Medium & High & Low \\
  Missing data handling & Yes & Yes & Yes & Yes \\
  Non-linear relationships & Yes & Yes & Yes & Yes \\
  Training complexity & Low & Medium & High & High \\
  Hyperparameter sensitivity & Low & Medium & High & Medium \\
  Interpretability & High & Medium & Low & Medium \\
  Memory requirements & Low & Medium & High & High \\\bottomrule
\end{tabular}}
\vspace{0.1cm}
{\small{\textbf{Note:} Partial uncertainty quantification in RCDNN
  refers to the use of Monte Carlo dropout for approximate
uncertainty estimation.}}
\end{table}

\section{Conclusion}\label{sec:cgp_conclusion}

In this chapter, we have presented a comprehensive overview of the
Chained Gaussian Processes model and its application to multiple
annotator learning. We have discussed the theoretical underpinnings of
the model, its implementation using GPflow and GPflux, and its
evaluation on synthetic datasets. It was shown in
\cite{GilGonzalezEtAl2023} that \gls{CCGPMA} outperforms other method
in state of the art methods in terms of accuracy and uncertainty
quantification (see \ref{subsec:bayesian_models}).